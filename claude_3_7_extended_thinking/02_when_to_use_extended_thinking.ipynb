{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8675320e-4d91-4c30-9a5a-d7ee0be907b8",
   "metadata": {},
   "source": [
    "# When and How to Use Extended Thinking\n",
    "\n",
    "In this notebook, we'll dive deeper into Claude 3.7 Sonnet's extended thinking capability, exploring:\n",
    "\n",
    "1. A task complexity classification framework\n",
    "2. A decision tree for when to use extended thinking\n",
    "3. Examples of appropriate use cases vs. cases where it's unnecessary\n",
    "4. Performance benchmarking on different task types\n",
    "5. Cost implications and optimization strategies\n",
    "\n",
    "By the end, you'll have a systematic approach to determine when extended thinking is beneficial and how to optimize its use for your specific applications.\n",
    "\n",
    "> **Note**: In this lesson, we're using the utility functions we developed in Lesson 1. The `claude_utils.py` module contains helper functions for creating Bedrock clients, invoking Claude with or without extended thinking, and displaying responses. This allows us to focus on the core concepts of when and how to use extended thinking rather than repeating boilerplate code.\n",
    ">\n",
    "> If you haven't completed Lesson 1 yet, you may want to review it first to understand how these utility functions work. Alternatively, you can examine the `claude_utils.py` file directly to see the implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4faee-702c-4b4f-9e08-377389236f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import claude_utils\n",
    "\n",
    "# Configure plot styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6548f2-ebb1-424b-b187-bf5a97a63bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Bedrock clients using our utility module\n",
    "REGION = 'us-west-2'  # Change to your preferred region\n",
    "bedrock, bedrock_runtime = claude_utils.create_bedrock_clients(REGION)\n",
    "\n",
    "# Claude 3.7 Sonnet model ID (consistent with Lesson 1)\n",
    "CLAUDE_37_SONNET_MODEL_ID = 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'\n",
    "\n",
    "# Verify model availability\n",
    "claude_utils.verify_model_availability(bedrock, CLAUDE_37_SONNET_MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba1b146-5e1f-4af2-b0bf-ff56978bc158",
   "metadata": {},
   "source": [
    "## 1. Task Complexity Classification Framework\n",
    "\n",
    "To determine when extended thinking is beneficial, we first need a framework to classify task complexity. This will help us make systematic decisions about when to use extended thinking and how much reasoning budget to allocate.\n",
    "\n",
    "Our framework classifies tasks into four levels of complexity:\n",
    "\n",
    "1. **Simple**: Straightforward factual queries, basic information retrieval, simple calculations\n",
    "2. **Medium**: Multi-step reasoning, moderate math problems, basic analysis tasks\n",
    "3. **Complex**: In-depth analysis, complex reasoning chains, constraint problems\n",
    "4. **Very Complex**: Systems design, advanced mathematical proofs, multi-stage problem solving\n",
    "\n",
    "Let's implement a classifier function that can automatically categorize tasks based on their complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e3c50-e166-4c9e-b8e3-30a17c21c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_task_complexity(prompt, model_id='us.anthropic.claude-3-5-haiku-20241022-v1:0'):\n",
    "    \"\"\"\n",
    "    Use Claude 3.5 Haiku to quickly classify the complexity of a task\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The user prompt to classify\n",
    "        model_id (str): The model ID to use for classification (defaults to Claude 3.5 Haiku for speed/cost)\n",
    "        \n",
    "    Returns:\n",
    "        str: Complexity classification ('simple', 'medium', 'complex', or 'very_complex')\n",
    "    \"\"\"\n",
    "    system_prompt = [\n",
    "        {\n",
    "            \"text\": \"\"\"You are a task complexity classifier. Classify the complexity of the given task into one of these categories: 'simple', 'medium', 'complex', or 'very_complex'. \n",
    "\n",
    "Here are examples of each complexity level:\n",
    "- simple: \"What is the capital of France?\", \"Calculate 25% of 80\", \"Summarize this short paragraph in one sentence\"\n",
    "- medium: \"A man has 53 socks in his drawer: 21 blue, 15 black and 17 red. How many socks must he take out to guarantee a black pair?\", \"Explain the greenhouse effect and its impact on climate\"\n",
    "- complex: \"Design a ride-sharing service that optimizes for driver availability and route efficiency\", \"Analyze the causes and economic impacts of the 2008 financial crisis\"\n",
    "- very_complex: \"Given a graph with n vertices and m edges, design an O(n+m) algorithm to find all bridges\", \"Design a quantum computing algorithm to solve the traveling salesman problem\"\n",
    "\n",
    "Respond with only the category name, nothing else.\"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": f\"Classify the complexity of this task: {prompt}\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            system=system_prompt,\n",
    "            inferenceConfig={\n",
    "                \"temperature\": 0,\n",
    "                \"maxTokens\": 10  # We only need a short response\n",
    "            }\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # Extract the classification\n",
    "        result = None\n",
    "        if response.get('output', {}).get('message', {}).get('content'):\n",
    "            content_blocks = response['output']['message']['content']\n",
    "            for block in content_blocks:\n",
    "                if 'text' in block:\n",
    "                    result = block['text'].strip().lower()\n",
    "                    break\n",
    "\n",
    "        # Ensure the result is one of our expected categories\n",
    "        valid_categories = ['simple', 'medium', 'complex', 'very_complex']\n",
    "        if result not in valid_categories:\n",
    "            result = 'medium'  # Default to medium if unexpected response\n",
    "\n",
    "        # Calculate approx cost\n",
    "        tokens = response['usage']['totalTokens']\n",
    "        cost = tokens * 0.00000125  # Assuming $0.00125 per 1K tokens for Haiku\n",
    "        \n",
    "        print(f\"Classification: {result} (determined in {elapsed_time:.2f}s, {tokens} tokens, ${cost:.6f})\")\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error classifying task complexity: {e}\")\n",
    "        return \"medium\"  # Default to medium complexity if there's an error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053aa889-9c84-4bae-b2b9-7d4a5ca7d37a",
   "metadata": {},
   "source": [
    "### Understanding the Task Complexity Classifier\n",
    "\n",
    "The `classify_task_complexity` function is an efficient way to automatically categorize the complexity of user prompts. Here's how it works:\n",
    "\n",
    "1. **Leveraging a smaller model**: We use **Claude 3.5 Haiku** instead of Claude 3.7 Sonnet for this classification step because it's faster and more cost-effective for this simple decision task.\n",
    "\n",
    "2. **Classification framework**: The function sends the prompt to Claude with explicit instructions defining four complexity categories (simple, medium, complex, very_complex) along with examples of each.\n",
    "\n",
    "3. **Efficiency considerations**: The function is optimized for speed and cost by:\n",
    "   - Using a smaller model **(Claude Haiku 3.5)**\n",
    "   - Setting temperature to 0 for deterministic responses\n",
    "   - Limiting the output to just 10 tokens\n",
    "   - Requesting only the category name\n",
    "\n",
    "4. **Practical application**: Think of this as the \"triage\" step in our workflow - similar to how a CPU scheduler determines how much processing time to allocate to different tasks. This initial assessment helps us allocate the appropriate \"thinking resources\" to the task at hand.\n",
    "\n",
    "This approach creates a \"thinking pipeline\" where we efficiently allocate Claude's reasoning capabilities based on task demands - using the right tool for each stage of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5c848-46ef-49c4-85d9-12ee00cba970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example tasks of varying complexity\n",
    "example_tasks = {\n",
    "    \"simple_1\": \"What is the capital of France?\",\n",
    "    \"simple_2\": \"What is 15% of 200?\",\n",
    "\n",
    "    \"medium_1\": \"A man has 53 socks in his drawer: 21 identical blue, 15 identical black and 17 identical red. The lights are out, and he is completely in the dark. How many socks must he take out to make 100 percent certain he has at least one pair of black socks?\",\n",
    "    \"medium_2\": \"Compare and contrast reinforcement learning and supervised learning in AI.\",\n",
    "\n",
    "    \"complex_1\": \"Design a system for a ride-sharing service that optimizes for driver availability, passenger wait times, and route efficiency. Include considerations for peak hours, variable pricing, and geographic distribution of drivers.\",\n",
    "    \"complex_2\": \"Analyze the causes and potential solutions to the prisoner's dilemma in game theory, including real-world applications and limitations.\",\n",
    "\n",
    "    \"very_complex_1\": \"Given a graph G with n vertices and m edges, design an algorithm to find all bridges in the graph in O(n+m) time. A bridge is defined as an edge whose removal increases the number of connected components in the graph. Provide pseudocode and explain why your algorithm achieves the desired time complexity.\",\n",
    "    \"very_complex_2\": \"Develop a comprehensive framework for a central bank to implement and manage a central bank digital currency (CBDC), addressing technological architecture, monetary policy implications, privacy concerns, and financial inclusion aspects.\"\n",
    "}\n",
    "\n",
    "# Test the classifier on our examples\n",
    "print(\"Testing Claude Haiku 3.5 task complexity classifier...\\n\")\n",
    "results = {}\n",
    "\n",
    "for label, task in example_tasks.items():\n",
    "    # Show abbreviated prompt if too long\n",
    "    display_prompt = task if len(task) < 100 else task[:97] + \"...\"\n",
    "    print(f\"Task {label}: \\\"{display_prompt}\\\"\")\n",
    "    complexity = classify_task_complexity(task)\n",
    "    results[label] = complexity\n",
    "    print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "# Display summary as a DataFrame\n",
    "#summary_df = pd.DataFrame([(k, v) for k, v in results.items()], \n",
    "#                        columns=['Task', 'Classified Complexity'])\n",
    "#display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd18257-a1d7-4f28-a565-f463d1c0b5d6",
   "metadata": {},
   "source": [
    "## 2. Decision Tree for When to Use Extended Thinking\n",
    "\n",
    "Based on our task complexity framework, we can create a decision tree to help determine:\n",
    "1. Whether to use extended thinking\n",
    "2. How much reasoning budget to allocate\n",
    "\n",
    "The decision tree takes into account:\n",
    "- Task complexity\n",
    "- Performance requirements\n",
    "- Time sensitivity\n",
    "- Cost considerations\n",
    "\n",
    "Here's a visualization of our decision tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaca51b8-cc6c-4685-959c-eb1c87be1346",
   "metadata": {},
   "source": [
    "![Decision Tree](./images/lesson2/complexity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709763cc-564c-4752-bde0-95df054e96eb",
   "metadata": {},
   "source": [
    "### Now, let's create a function to automatically determine whether to use extended thinking and what budget to allocate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca67398-da0c-4061-b616-01b6b39946c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_extended_thinking_strategy(prompt, time_sensitive=False):\n",
    "    \"\"\"\n",
    "    Determine whether to use extended thinking and what budget to allocate\n",
    "    based on task complexity and time sensitivity\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The user prompt\n",
    "        time_sensitive (bool): Whether the task is time-sensitive\n",
    "        \n",
    "    Returns:\n",
    "        dict: Strategy with 'use_extended_thinking' and 'reasoning_budget' keys\n",
    "    \"\"\"\n",
    "    # First, classify the task complexity\n",
    "    complexity = classify_task_complexity(prompt)\n",
    "    \n",
    "    # Define reasoning budget ranges for each complexity level\n",
    "    budget_ranges = {\n",
    "        'simple': (0, 0),  # No extended thinking for simple tasks\n",
    "        'medium': (1024, 2048),\n",
    "        'complex': (2048, 8192),\n",
    "        'very_complex': (8192, 16384)\n",
    "    }\n",
    "    \n",
    "    # Determine whether to use extended thinking based on complexity and time sensitivity\n",
    "    use_extended_thinking = True\n",
    "    \n",
    "    if complexity == 'simple':\n",
    "        use_extended_thinking = False\n",
    "    elif complexity == 'medium' and time_sensitive:\n",
    "        use_extended_thinking = False\n",
    "    \n",
    "    # Determine reasoning budget (if using extended thinking)\n",
    "    if use_extended_thinking:\n",
    "        min_budget, max_budget = budget_ranges[complexity]\n",
    "        \n",
    "        # Use the lower end of the range if time_sensitive, otherwise use the middle\n",
    "        if time_sensitive:\n",
    "            reasoning_budget = min_budget\n",
    "        else:\n",
    "            reasoning_budget = (min_budget + max_budget) // 2\n",
    "    else:\n",
    "        reasoning_budget = 0\n",
    "    \n",
    "    strategy = {\n",
    "        'complexity': complexity,\n",
    "        'use_extended_thinking': use_extended_thinking,\n",
    "        'reasoning_budget': reasoning_budget,\n",
    "        'time_sensitive': time_sensitive\n",
    "    }\n",
    "    \n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b463c7-2dbd-4844-80f2-0f483e6ddab5",
   "metadata": {},
   "source": [
    "### Understanding the Extended Thinking Strategy Function\n",
    "\n",
    "The `determine_extended_thinking_strategy` function acts as an automated decision-making system that applies our decision tree logic, first classifying task complexity and then determining whether to use extended thinking and what reasoning budget to allocate based on both complexity and time sensitivity. Like a smart resource manager, it efficiently routes tasks to the appropriate processing pipeline with the right amount of \"thinking power\" based on the task's demands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a74ea-c51b-4b69-94f2-abed90f771dc",
   "metadata": {},
   "source": [
    "## 3. Examples of Appropriate Use Cases vs. Cases Where Extended Thinking is Unnecessary\n",
    "\n",
    "Now that we have our framework and decision tree, let's examine specific examples of when extended thinking is beneficial and when it's unnecessary. We'll test both scenarios with real prompts and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1376b98-da2c-4970-b672-4825b41d464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_and_without_extended_thinking(prompt, max_tokens=500):\n",
    "    \"\"\"\n",
    "    Test a prompt with and without extended thinking and compare the results\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt to test\n",
    "        max_tokens (int): Maximum tokens for the response\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results including both responses and metrics\n",
    "    \"\"\"\n",
    "    print(f\"Testing prompt: {prompt[:100]}...\" if len(prompt) > 100 else f\"Testing prompt: {prompt}\")\n",
    "    \n",
    "    # Determine optimal strategy\n",
    "    strategy = determine_extended_thinking_strategy(prompt)\n",
    "    print(f\"Strategy: Complexity={strategy['complexity']}, Use Extended Thinking={strategy['use_extended_thinking']}, Budget={strategy['reasoning_budget']}\")\n",
    "    \n",
    "    # Test without extended thinking\n",
    "    print(\"\\nTesting WITHOUT extended thinking...\")\n",
    "    standard_response = claude_utils.invoke_claude(\n",
    "        bedrock_runtime,\n",
    "        prompt, \n",
    "        CLAUDE_37_SONNET_MODEL_ID, \n",
    "        enable_reasoning=False,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    # Test with extended thinking (if recommended)\n",
    "    reasoning_response = None\n",
    "    if strategy['use_extended_thinking']:\n",
    "        print(\"\\nTesting WITH extended thinking...\")\n",
    "        reasoning_response = claude_utils.invoke_claude(\n",
    "            bedrock_runtime,\n",
    "            prompt, \n",
    "            CLAUDE_37_SONNET_MODEL_ID, \n",
    "            enable_reasoning=True,\n",
    "            reasoning_budget=strategy['reasoning_budget'],\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "    \n",
    "    # Display results\n",
    "    standard_result = claude_utils.extract_response_content(standard_response)\n",
    "    print(\"\\n--- Standard Mode Result ---\")\n",
    "    standard_time = standard_response.get('_elapsed_time', 0)\n",
    "    standard_tokens = standard_response.get('usage', {}).get('totalTokens', 0)\n",
    "    standard_cost = (standard_response.get('usage', {}).get('inputTokens', 0) * 0.000003) + \\\n",
    "                   (standard_response.get('usage', {}).get('outputTokens', 0) * 0.000005)\n",
    "    \n",
    "    print(f\"Time: {standard_time:.2f}s, Tokens: {standard_tokens}, Cost: ${standard_cost:.6f}\")\n",
    "    display(Markdown(f\"**Standard Mode Result:**\\n{standard_result[:500]}...\"))\n",
    "    \n",
    "    # Only show extended thinking results if we used it\n",
    "    reasoning_result = None\n",
    "    if reasoning_response:\n",
    "        reasoning_result = claude_utils.extract_response_content(reasoning_response)\n",
    "        print(\"\\n--- Extended Thinking Mode Result ---\")\n",
    "        reasoning_time = reasoning_response.get('_elapsed_time', 0)\n",
    "        reasoning_tokens = reasoning_response.get('usage', {}).get('totalTokens', 0)\n",
    "        reasoning_cost = (reasoning_response.get('usage', {}).get('inputTokens', 0) * 0.000003) + \\\n",
    "                        (reasoning_response.get('usage', {}).get('outputTokens', 0) * 0.000005)\n",
    "        \n",
    "        print(f\"Time: {reasoning_time:.2f}s, Tokens: {reasoning_tokens}, Cost: ${reasoning_cost:.6f}\")\n",
    "        display(Markdown(f\"**Extended Thinking Mode Result:**\\n{reasoning_result[:500]}...\"))\n",
    "    \n",
    "    return {\n",
    "        'strategy': strategy,\n",
    "        'standard_response': standard_response,\n",
    "        'reasoning_response': reasoning_response,\n",
    "        'standard_result': standard_result,\n",
    "        'reasoning_result': reasoning_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f028c0fc-0697-4901-8a15-efd5d547c555",
   "metadata": {},
   "source": [
    "### Now let's test some appropriate and inappropriate use cases for extended thinking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb17d40-4fd9-4831-94dd-7e09cb3aceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple factual query (should NOT use extended thinking)\n",
    "simple_query = \"What are the three primary colors of light?\"\n",
    "simple_results = test_with_and_without_extended_thinking(simple_query)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Example 2: Complex reasoning task (SHOULD use extended thinking)\n",
    "complex_query = \"\"\"\n",
    "Analyze the knapsack problem in computer science. Explain the dynamic programming approach \n",
    "to solve it, provide pseudocode, and analyze the time and space complexity. \n",
    "Also explain when a greedy approach might work and when it would fail.\n",
    "\"\"\"\n",
    "complex_results = test_with_and_without_extended_thinking(complex_query, max_tokens=800)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Example 3: Medium complexity task (may use extended thinking if not time-sensitive)\n",
    "medium_query = \"\"\"\n",
    "Compare and contrast supervised learning and unsupervised learning in machine learning. \n",
    "Give examples of algorithms in each category and scenarios where one would be preferred over the other.\n",
    "\"\"\"\n",
    "medium_results = test_with_and_without_extended_thinking(medium_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4092385-54ff-41e2-8507-4e2114f7f635",
   "metadata": {},
   "source": [
    "## 4. Performance Benchmarking on Different Task Types\n",
    "\n",
    "Now that we've seen individual examples, let's systematically benchmark Claude's performance across different task types with and without extended thinking. We'll measure several key metrics:\n",
    "\n",
    "1. **Response quality** (qualitative assessment)\n",
    "2. **Time to generate response** (latency)\n",
    "3. **Token usage** (and associated cost)\n",
    "4. **Efficiency** (tokens per second)\n",
    "\n",
    "This benchmarking will help us quantify the tradeoffs between using extended thinking and standard mode for different task types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc366c-4fcc-4efc-aa28-efac1a1dd563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmarking_comparison(tasks, max_tokens=500):\n",
    "    \"\"\"\n",
    "    Run a systematic benchmarking comparison across different task types\n",
    "    \n",
    "    Args:\n",
    "        tasks (dict): Dictionary of task labels to prompts\n",
    "        max_tokens (int): Maximum tokens for responses\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Benchmarking results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for label, prompt in tasks.items():\n",
    "        print(f\"\\nBenchmarking task: {label}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # 1. Test without extended thinking\n",
    "        print(f\"Testing standard mode...\")\n",
    "        standard_start = time.time()\n",
    "        standard_response = claude_utils.invoke_claude(\n",
    "            bedrock_runtime,\n",
    "            prompt, \n",
    "            CLAUDE_37_SONNET_MODEL_ID, \n",
    "            enable_reasoning=False,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        standard_time = time.time() - standard_start\n",
    "        \n",
    "        standard_input_tokens = standard_response.get('usage', {}).get('inputTokens', 0)\n",
    "        standard_output_tokens = standard_response.get('usage', {}).get('outputTokens', 0)\n",
    "        standard_total_tokens = standard_response.get('usage', {}).get('totalTokens', 0)\n",
    "        standard_cost = (standard_input_tokens * 0.000003) + (standard_output_tokens * 0.000005)\n",
    "        \n",
    "        # 2. Test with extended thinking\n",
    "        print(f\"Testing extended thinking mode...\")\n",
    "        # Determine appropriate budget based on complexity\n",
    "        complexity = classify_task_complexity(prompt)\n",
    "        budget_map = {\n",
    "            'simple': 1024,\n",
    "            'medium': 2048,\n",
    "            'complex': 4096,\n",
    "            'very_complex': 8192\n",
    "        }\n",
    "        budget = budget_map.get(complexity, 2048)\n",
    "        \n",
    "        reasoning_start = time.time()\n",
    "        reasoning_response = claude_utils.invoke_claude(\n",
    "            bedrock_runtime,\n",
    "            prompt, \n",
    "            CLAUDE_37_SONNET_MODEL_ID, \n",
    "            enable_reasoning=True,\n",
    "            reasoning_budget=budget,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        reasoning_time = time.time() - reasoning_start\n",
    "        \n",
    "        reasoning_input_tokens = reasoning_response.get('usage', {}).get('inputTokens', 0)\n",
    "        reasoning_output_tokens = reasoning_response.get('usage', {}).get('outputTokens', 0)\n",
    "        reasoning_total_tokens = reasoning_response.get('usage', {}).get('totalTokens', 0)\n",
    "        reasoning_cost = (reasoning_input_tokens * 0.000003) + (reasoning_output_tokens * 0.000005)\n",
    "        \n",
    "        # Calculate efficiency metrics\n",
    "        standard_efficiency = standard_total_tokens / standard_time if standard_time > 0 else 0\n",
    "        reasoning_efficiency = reasoning_total_tokens / reasoning_time if reasoning_time > 0 else 0\n",
    "        \n",
    "        # Collect results\n",
    "        result = {\n",
    "            'Task': label,\n",
    "            'Complexity': complexity,\n",
    "            'Standard_Time': standard_time,\n",
    "            'Standard_Tokens': standard_total_tokens,\n",
    "            'Standard_Cost': standard_cost,\n",
    "            'Standard_Efficiency': standard_efficiency,\n",
    "            'Reasoning_Time': reasoning_time,\n",
    "            'Reasoning_Tokens': reasoning_total_tokens,\n",
    "            'Reasoning_Cost': reasoning_cost,\n",
    "            'Reasoning_Efficiency': reasoning_efficiency,\n",
    "            'Time_Ratio': reasoning_time / standard_time if standard_time > 0 else 0,\n",
    "            'Cost_Ratio': reasoning_cost / standard_cost if standard_cost > 0 else 0\n",
    "        }\n",
    "        \n",
    "        print(f\"Results: {complexity} task\")\n",
    "        print(f\"Standard: {standard_time:.2f}s, {standard_total_tokens} tokens, ${standard_cost:.6f}\")\n",
    "        print(f\"Reasoning: {reasoning_time:.2f}s, {reasoning_total_tokens} tokens, ${reasoning_cost:.6f}\")\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "# Define benchmark tasks\n",
    "benchmark_tasks = {\n",
    "    \"FactualQuery\": \"What are the major planets in our solar system?\",\n",
    "    \"SimpleCalc\": \"If I have 5 apples and give away 2, then buy 3 more, how many do I have?\",\n",
    "    \"MediumAnalysis\": \"Compare the advantages and disadvantages of renewable energy vs. fossil fuels.\",\n",
    "    \"SockDrawer\": \"A man has 53 socks in his drawer: 21 identical blue, 15 identical black and 17 identical red. The lights are out, and he is completely in the dark. How many socks must he take out to make 100 percent certain he has at least one pair of black socks?\",\n",
    "    \"ComplexDesign\": \"Design a system for coordinating autonomous delivery drones in an urban environment, considering obstacles, weather, traffic patterns, and regulatory compliance.\",\n",
    "    \"AdvancedCoding\": \"Explain how you would implement a distributed system for real-time processing of financial transactions that ensures ACID properties while maintaining high throughput.\"\n",
    "}\n",
    "\n",
    "# Run the benchmarking comparison\n",
    "benchmark_results = run_benchmarking_comparison(benchmark_tasks)\n",
    "\n",
    "# Display results\n",
    "display(benchmark_results)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# 1. Time comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "benchmark_results.plot(kind='bar', x='Task', y=['Standard_Time', 'Reasoning_Time'], ax=plt.gca())\n",
    "plt.title('Response Time Comparison')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 2. Cost comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "benchmark_results.plot(kind='bar', x='Task', y=['Standard_Cost', 'Reasoning_Cost'], ax=plt.gca())\n",
    "plt.title('Cost Comparison')\n",
    "plt.ylabel('Cost ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 3. Efficiency comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "benchmark_results.plot(kind='bar', x='Task', y=['Standard_Efficiency', 'Reasoning_Efficiency'], ax=plt.gca())\n",
    "plt.title('Efficiency Comparison (Tokens/Second)')\n",
    "plt.ylabel('Tokens per Second')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 4. Ratio analysis by complexity\n",
    "plt.subplot(2, 2, 4)\n",
    "complexity_order = ['simple', 'medium', 'complex', 'very_complex']\n",
    "benchmark_results['Complexity'] = pd.Categorical(benchmark_results['Complexity'], categories=complexity_order, ordered=True)\n",
    "benchmark_results.sort_values('Complexity', inplace=True)\n",
    "\n",
    "# Create separate scatter plots for each ratio instead of trying to plot both at once\n",
    "benchmark_results.plot(kind='scatter', x='Complexity', y='Time_Ratio', color='blue', label='Time Ratio', ax=plt.gca())\n",
    "benchmark_results.plot(kind='scatter', x='Complexity', y='Cost_Ratio', color='red', label='Cost Ratio', ax=plt.gca())\n",
    "\n",
    "plt.title('Extended Thinking/Standard Ratios by Complexity')\n",
    "plt.ylabel('Ratio (Extended/Standard)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4163f2-a301-44ae-b0a0-1eebb21f29a3",
   "metadata": {},
   "source": [
    "### Understanding the Benchmarking Function\n",
    "\n",
    "The `run_benchmarking_comparison` function serves as our experimental lab, systematically testing Claude's performance across different task types both with and without extended thinking. For each task, it measures response time, token usage, cost, and efficiency metrics, allowing us to quantify the exact performance tradeoffs and determine where extended thinking provides the most value. Think of it as a controlled experiment that helps us see beyond anecdotal evidence to develop data-driven guidelines for when to use extended thinking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5ef21c-bcb7-4205-bbd2-1a880762b2c1",
   "metadata": {},
   "source": [
    "## 5. Cost Implications and Optimization Strategies\n",
    "\n",
    "Based on our benchmarking results, we can develop some strategies to optimize the cost-performance tradeoff when using Claude's extended thinking capabilities.\n",
    "\n",
    "### Cost Structure\n",
    "\n",
    "Claude 3.7 Sonnet's pricing is consistent across both standard and extended thinking modes:\n",
    "- **Input tokens**: $3 per million tokens\n",
    "- **Output tokens**: $15 per million tokens (including thinking tokens)\n",
    "\n",
    "This means that extended thinking primarily increases costs through additional output tokens used for reasoning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de32a0-940c-42c0-9be3-0b5dd23c899c",
   "metadata": {},
   "source": [
    "### Optimization Strategies\n",
    "\n",
    "Let's explore several strategies to optimize the cost-benefit tradeoff:\n",
    "\n",
    "| Strategy | Description | When to Use |\n",
    "|----------|-------------|-------------|\n",
    "| Task Complexity Filtering | Only use extended thinking for complex and very complex tasks | Apply our classifier to route simple tasks to standard mode |\n",
    "| Dynamic Budget Allocation | Allocate reasoning budget based on task complexity | Scale budget from 1024 tokens (minimum) to 16384+ tokens based on complexity |\n",
    "| Two-phase Approach | Use standard mode first, only invoke extended thinking if needed | For uncertain cases or when standard mode confidence is low |\n",
    "| Batch Similar Tasks | Group similar tasks to amortize the classification cost | For applications processing many similar requests (e.g., customer service) |\n",
    "| Reasoning Budget Caps | Set upper limits on reasoning budgets based on ROI analysis | When diminishing returns are observed at higher budgets |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84583941-4c77-4397-af4c-85e1facfc175",
   "metadata": {},
   "source": [
    "### Implementation Example: Cost-Optimized Extended Thinking\n",
    "\n",
    "Below is an implementation of an optimized approach that balances cost and performance. This function:\n",
    "\n",
    "1. Uses our complexity classifier to determine if extended thinking is needed\n",
    "2. Allocates an appropriate reasoning budget based on complexity\n",
    "3. Enforces budget caps to prevent excessive token usage\n",
    "4. Provides transparency through detailed monitoring metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a95cc4-9cf5-4e59-ab7b-7f5ea51d7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_optimized_invoke(prompt, time_sensitive=False, max_tokens=1000):\n",
    "    \"\"\"\n",
    "    Invoke Claude with cost-optimized extended thinking\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The user prompt\n",
    "        time_sensitive (bool): Whether the task is time-sensitive\n",
    "        max_tokens (int): Maximum tokens for the final response\n",
    "        \n",
    "    Returns:\n",
    "        dict: Response and performance metrics\n",
    "    \"\"\"\n",
    "    # Step 1: Determine strategy\n",
    "    strategy = determine_extended_thinking_strategy(prompt, time_sensitive=time_sensitive)\n",
    "    \n",
    "    # Step 2: Apply budget caps based on ROI analysis\n",
    "    # (These caps would ideally be determined through extensive benchmarking)\n",
    "    budget_caps = {\n",
    "        'simple': 0,\n",
    "        'medium': 2048,\n",
    "        'complex': 4096,\n",
    "        'very_complex': 8192\n",
    "    }\n",
    "    \n",
    "    if strategy['use_extended_thinking']:\n",
    "        strategy['reasoning_budget'] = min(strategy['reasoning_budget'], budget_caps[strategy['complexity']])\n",
    "    \n",
    "    # Step 3: Invoke Claude with the optimized settings\n",
    "    response = claude_utils.invoke_claude(\n",
    "        bedrock_runtime,\n",
    "        prompt,\n",
    "        CLAUDE_37_SONNET_MODEL_ID,\n",
    "        enable_reasoning=strategy['use_extended_thinking'],\n",
    "        reasoning_budget=strategy['reasoning_budget'],\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    # Step 4: Calculate performance metrics\n",
    "    elapsed_time = response.get('_elapsed_time', 0)\n",
    "    input_tokens = response.get('usage', {}).get('inputTokens', 0)\n",
    "    output_tokens = response.get('usage', {}).get('outputTokens', 0)\n",
    "    total_tokens = response.get('usage', {}).get('totalTokens', 0)\n",
    "    \n",
    "    input_cost = input_tokens * 0.000003\n",
    "    output_cost = output_tokens * 0.000005\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    metrics = {\n",
    "        'strategy': strategy,\n",
    "        'elapsed_time': elapsed_time,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'total_tokens': total_tokens,\n",
    "        'input_cost': input_cost,\n",
    "        'output_cost': output_cost,\n",
    "        'total_cost': total_cost\n",
    "    }\n",
    "    \n",
    "    # Add the metrics to the response for transparency\n",
    "    response['_metrics'] = metrics\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Demonstrate the cost-optimized approach\n",
    "print(\"Testing cost-optimized approach...\")\n",
    "test_prompt = \"\"\"\n",
    "A spaceship needs to visit 5 planets (A, B, C, D, and E) starting from Earth and then returning to Earth. \n",
    "The distances between each pair of locations are as follows (in light-years):\n",
    "Earth-A: 5, Earth-B: 7, Earth-C: 8, Earth-D: 10, Earth-E: 12\n",
    "A-B: 4, A-C: 6, A-D: 9, A-E: 11\n",
    "B-C: 5, B-D: 8, B-E: 10\n",
    "C-D: 6, C-E: 9\n",
    "D-E: 7\n",
    "\n",
    "What's the shortest possible route that visits each planet exactly once before returning to Earth?\n",
    "\"\"\"\n",
    "\n",
    "cost_optimized_response = cost_optimized_invoke(test_prompt)\n",
    "claude_utils.display_claude_response(cost_optimized_response)\n",
    "\n",
    "# Display the optimization metrics\n",
    "metrics = cost_optimized_response.get('_metrics', {})\n",
    "print(\"\\nOptimization Metrics:\")\n",
    "print(f\"Task Complexity: {metrics.get('strategy', {}).get('complexity', 'unknown')}\")\n",
    "print(f\"Extended Thinking Used: {metrics.get('strategy', {}).get('use_extended_thinking', False)}\")\n",
    "print(f\"Reasoning Budget: {metrics.get('strategy', {}).get('reasoning_budget', 0)} tokens\")\n",
    "print(f\"Time: {metrics.get('elapsed_time', 0):.2f} seconds\")\n",
    "print(f\"Cost: ${metrics.get('total_cost', 0):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3288d0-95ab-4782-ab7c-c21aaba025f5",
   "metadata": {},
   "source": [
    "## Conclusion: A Framework for Effective Use of Extended Thinking\n",
    "\n",
    "We've developed a systematic framework for determining when to use Claude's extended thinking capability and how to optimize its use:\n",
    "\n",
    "1. **Classify task complexity** using our automated classifier\n",
    "2. **Apply our decision tree** to determine if extended thinking is needed\n",
    "3. **Allocate appropriate reasoning budget** based on complexity\n",
    "4. **Monitor performance metrics** to continuously refine the approach\n",
    "\n",
    "By following this framework, you can:\n",
    "- Improve response quality for complex tasks\n",
    "- Avoid unnecessary costs for simple tasks\n",
    "- Balance performance and cost considerations\n",
    "- Adapt the approach based on your specific requirements\n",
    "\n",
    "In the next notebook, we'll explore how to optimize reasoning budget allocation in more detail, building on the foundation established here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
