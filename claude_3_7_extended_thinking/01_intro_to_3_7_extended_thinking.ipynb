{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a36d098-0dd7-4a60-9fa4-df879a893cf2",
   "metadata": {},
   "source": [
    "# Introduction to Claude 3.7 Sonnet and Extended Thinking\n",
    "\n",
    "This notebook provides an introduction to Anthropic's Claude 3.7 Sonnet model and its innovative \"extended thinking\" capability. We'll explore:\n",
    "\n",
    "1. Overview of Claude 3.7 Sonnet's capabilities\n",
    "2. Understanding extended thinking and how it works\n",
    "3. Setting up Claude 3.7 in Amazon Bedrock\n",
    "4. Comparing standard mode vs. extended thinking mode\n",
    "5. Visualizing the thinking process\n",
    "\n",
    "By the end of this notebook, you'll have a clear understanding of when and how to use extended thinking to improve your AI workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f62af895-abaa-473d-944a-d0db857bbdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt -qU --disable-pip-version-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165fa07-26e3-4343-9f53-7f5f9b7d62a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb4ea05-3d2e-4326-88d0-92edef095d97",
   "metadata": {},
   "source": [
    "## 1. Overview of Claude 3.7 Sonnet's Capabilities\n",
    "\n",
    "Claude 3.7 Sonnet represents Anthropic's most advanced model to date, released in February 2025. It introduces several key improvements over previous Claude models:\n",
    "\n",
    "### Key Capabilities\n",
    "\n",
    "- **Hybrid Reasoning Approach**: Claude 3.7 Sonnet can operate in both standard mode and \"extended thinking\" mode, allowing you to control when the model engages in deeper reasoning.\n",
    "\n",
    "- **Increased Output Length**: Supports up to 64K output tokens (8x longer than previous models), with up to 128K tokens in preview.\n",
    "\n",
    "- **Enhanced Computer Use**: Improved capabilities for computer interaction, including additional actions like scroll, wait, left mouse down/up, hold key, and triple click.\n",
    "\n",
    "- **Improved Code Generation**: Leading performance on coding benchmarks, particularly with SWE-bench Verified.\n",
    "\n",
    "- **Reasoning Budget Control**: When using the API, you can control exactly how much \"thinking power\" to allocate to a task, from the minimum required budget to much larger allocations for complex problems.\n",
    "\n",
    "Claude 3.7 Sonnet excels at tasks requiring deep analysis, complex problem-solving, and multi-step reasoning - while maintaining the ability to provide quick responses when extended thinking isn't needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74121f3-55e7-4e44-b02a-41e7843af7a3",
   "metadata": {},
   "source": [
    "## 2. Understanding Extended Thinking\n",
    "\n",
    "### What is Extended Thinking?\n",
    "\n",
    "Extended thinking is a groundbreaking capability that allows Claude to solve complex problems through step-by-step reasoning that is visible to the user. Think of it like watching a mathematician work through a proof on a whiteboard, rather than just seeing the final answer.\n",
    "\n",
    "### How Extended Thinking Works\n",
    "\n",
    "When enabled, Claude 3.7 Sonnet follows a process similar to human problem-solving:\n",
    "\n",
    "1. It first processes the task in an internal \"scratchpad\" - thinking through the problem step by step\n",
    "2. This reasoning process is visible to you in the API response\n",
    "3. After completing its reasoning, Claude provides a final answer informed by this thinking\n",
    "\n",
    "### Extended Thinking vs. Chain of Thought\n",
    "\n",
    "**Traditional Chain of Thought (CoT):**\n",
    "- Requires specific prompting to elicit step-by-step reasoning\n",
    "- The reasoning quality depends heavily on prompt engineering\n",
    "- Reasoning and response are intermingled\n",
    "- No control over reasoning depth\n",
    "\n",
    "**Extended Thinking:**\n",
    "- Explicitly enabled via API parameter\n",
    "- Reasoning budget can be precisely controlled\n",
    "- Reasoning appears in a separate field from the final response\n",
    "- More thorough and structured reasoning\n",
    "\n",
    "### Reasoning Budget\n",
    "\n",
    "A key innovation of Claude 3.7 Sonnet is the ability to control the \"reasoning budget\" - the amount of tokens allocated to the thinking process:\n",
    "\n",
    "- Minimum budget: 1,024 tokens\n",
    "- Can be increased up to the model's 128K token limit\n",
    "- Larger budgets allow for more thorough reasoning on complex problems\n",
    "\n",
    "Think of the reasoning budget like allocating CPU time to a computational task - more complex tasks benefit from larger allocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d7a97-436d-4161-bd83-3514d4d00197",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Setting up Claude 3.7 in Amazon Bedrock\n",
    "\n",
    "# Configure the AWS region\n",
    "REGION = 'us-west-2'  # Change to your preferred region\n",
    "\n",
    "# Initialize Bedrock clients\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    region_name=REGION,\n",
    ")\n",
    "\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=REGION,\n",
    ")\n",
    "\n",
    "# Claude 3.7 Sonnet model ID\n",
    "CLAUDE_37_SONNET_MODEL_ID = 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'\n",
    "\n",
    "# For comparison, Claude 3.5 Sonnet model ID\n",
    "CLAUDE_35_SONNET_MODEL_ID = 'us.anthropic.claude-3-5-sonnet-20241022-v2:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3373568-7a3e-4fd2-9ad7-c2133f32a362",
   "metadata": {},
   "source": [
    "> **Note**: In this lesson, we're showing you the fucntions below for clarity, but going forward we will make these utility functions imported as `claude_utils`. The `claude_utils.py` module contains helper functions for creating Bedrock clients, invoking Claude with or without extended thinking, and displaying responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46d082-9c69-46d2-8727-3c7446c26858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_claude(\n",
    "    prompt, \n",
    "    model_id=CLAUDE_37_SONNET_MODEL_ID, \n",
    "    enable_reasoning=False, \n",
    "    reasoning_budget=1024,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Invoke Claude with or without extended thinking.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt to send to Claude\n",
    "        model_id (str): The model ID to use\n",
    "        enable_reasoning (bool): Whether to enable extended thinking\n",
    "        reasoning_budget (int): Token budget for reasoning (min 1024)\n",
    "        temperature (float): Temperature for generation (0.0-1.0)\n",
    "        max_tokens (int): Maximum tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        dict: The complete API response\n",
    "    \"\"\"\n",
    "    # Create system prompt and messages\n",
    "    system_prompt = [{\"text\": \"You're a helpful AI assistant.\"}]\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": prompt}]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Base request parameters\n",
    "    request_params = {\n",
    "        \"modelId\": model_id,\n",
    "        \"messages\": messages,\n",
    "        \"system\": system_prompt,\n",
    "        \"inferenceConfig\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add reasoning configuration if enabled\n",
    "    if enable_reasoning:\n",
    "        # When using reasoning, temperature must be 1.0\n",
    "        request_params[\"inferenceConfig\"][\"temperature\"] = 1.0\n",
    "        \n",
    "        # Ensure maxTokens is greater than reasoning_budget\n",
    "        if max_tokens <= reasoning_budget:\n",
    "            # Make it just one token more than the reasoning budget\n",
    "            adjusted_max_tokens = reasoning_budget + 1\n",
    "            print(f\"Info: Increasing maxTokens from {max_tokens} to {adjusted_max_tokens} to exceed reasoning budget\")\n",
    "            request_params[\"inferenceConfig\"][\"maxTokens\"] = adjusted_max_tokens\n",
    "        \n",
    "        request_params[\"additionalModelRequestFields\"] = {\n",
    "            \"reasoning_config\": {\n",
    "                \"type\": \"enabled\",\n",
    "                \"budget_tokens\": reasoning_budget\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Invoke the model\n",
    "    start_time = time.time()\n",
    "    response = bedrock_runtime.converse(**request_params)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Add elapsed time to response for reference\n",
    "    response[\"_elapsed_time\"] = elapsed_time\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def extract_response_content(response):\n",
    "    \"\"\"Extract the response content from Claude's API response\"\"\"\n",
    "    if response.get('output', {}).get('message', {}).get('content'):\n",
    "        content_blocks = response['output']['message']['content']\n",
    "        for block in content_blocks:\n",
    "            if 'text' in block:\n",
    "                return block['text']\n",
    "    return \"No response content found\"\n",
    "\n",
    "def display_claude_response(response, show_reasoning=False):\n",
    "    \"\"\"Display Claude's response in a nicely formatted way\"\"\"\n",
    "    result = extract_response_content(response)\n",
    "    \n",
    "    # Calculate costs (approximate)\n",
    "    input_tokens = response.get('usage', {}).get('inputTokens', 0)\n",
    "    output_tokens = response.get('usage', {}).get('outputTokens', 0)\n",
    "    total_tokens = response.get('usage', {}).get('totalTokens', 0)\n",
    "    \n",
    "    input_cost = input_tokens * 0.000003  # $3 per million tokens\n",
    "    output_cost = output_tokens * 0.000015  # $15 per million tokens\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    # Display metrics\n",
    "    display(Markdown(f\"### Response (in {response.get('_elapsed_time', 0):.2f} seconds)\"))\n",
    "    display(Markdown(f\"**Tokens**: {total_tokens:,} total ({input_tokens:,} input, {output_tokens:,} output)\"))\n",
    "    display(Markdown(f\"**Estimated cost**: ${total_cost:.5f}\"))\n",
    "    \n",
    "    # In the final implementation, there would be a proper way to access the reasoning content\n",
    "    # but we're not implementing that workaround here as instructed\n",
    "    \n",
    "    # Display the actual response\n",
    "    display(Markdown(\"### Claude's Response:\"))\n",
    "    display(Markdown(result))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72260615-b301-4f83-92ce-1e390a2c4db1",
   "metadata": {},
   "source": [
    "## 4. Comparing Standard Mode vs. Extended Thinking Mode\n",
    "\n",
    "Now let's see Claude 3.7 Sonnet in action, comparing its performance with and without extended thinking enabled. We'll test it on a few different types of problems to demonstrate when extended thinking provides the most benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e7a36-4f70-4fae-9a46-e68a77326719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example - Capital city question\n",
    "simple_prompt = \"What is the capital of France?\"\n",
    "\n",
    "# Without extended thinking\n",
    "print(\"Calling Claude 3.7 Sonnet WITHOUT extended thinking...\")\n",
    "standard_response = invoke_claude(\n",
    "    simple_prompt,\n",
    "    enable_reasoning=False,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "display_claude_response(standard_response, show_reasoning=False)\n",
    "\n",
    "# With extended thinking\n",
    "print(\"\\nCalling Claude 3.7 Sonnet WITH extended thinking...\")\n",
    "reasoning_response = invoke_claude(\n",
    "    simple_prompt,\n",
    "    enable_reasoning=True,\n",
    "    reasoning_budget=1024,  # Minimum budget\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "display_claude_response(reasoning_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bf8193-6ba1-48cd-9fb1-114f2022bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex example - Sock drawer problem\n",
    "complex_prompt = \"\"\"\n",
    "A man has 53 socks in his drawer: 21 identical blue, 15 identical black and 17 identical red.\n",
    "The lights are out, and he is completely in the dark. How many socks must he take out to make 100 percent\n",
    "certain he has at least one pair of black socks?\n",
    "\"\"\"\n",
    "\n",
    "# Without extended thinking\n",
    "print(\"Calling Claude 3.7 Sonnet WITHOUT extended thinking...\")\n",
    "standard_complex_response = invoke_claude(\n",
    "    complex_prompt,\n",
    "    enable_reasoning=False,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "display_claude_response(standard_complex_response, show_reasoning=False)\n",
    "\n",
    "# With extended thinking\n",
    "print(\"\\nCalling Claude 3.7 Sonnet WITH extended thinking...\")\n",
    "reasoning_complex_response = invoke_claude(\n",
    "    complex_prompt,\n",
    "    enable_reasoning=True,\n",
    "    reasoning_budget=2048,  # A bit more budget for this problem\n",
    "    max_tokens=2049  # Ensuring it's at least 1 token more than the reasoning budget\n",
    ")\n",
    "\n",
    "display_claude_response(reasoning_complex_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c98b46-f5ce-454e-ab57-02f3ac825823",
   "metadata": {},
   "source": [
    "## 5. Analyzing Performance Across Different Reasoning Budgets\n",
    "\n",
    "Now let's systematically analyze how different reasoning budgets affect Claude's performance on our sock drawer problem. We'll test four different budget sizes:\n",
    "\n",
    "- **1,024 tokens**: The minimum required budget\n",
    "- **2,048 tokens**: A moderate budget\n",
    "- **4,096 tokens**: A generous budget\n",
    "- **8,192 tokens**: A very large budget\n",
    "\n",
    "For each budget size, we'll measure:\n",
    "1. **Response time**: How long it takes to get a response\n",
    "2. **Token usage**: Total tokens used (input + output)\n",
    "3. **Cost**: Estimated cost based on token usage\n",
    "4. **Efficiency**: Tokens processed per second\n",
    "\n",
    "This analysis will help us find the optimal reasoning budget that balances cost, speed, and performance. Think of it like finding the right amount of CPU time to allocate to a computational task - too little and the model might not have enough \"thinking space\" to solve the problem effectively, too much and you're wasting resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c234a2a2-e0a6-49c8-8dce-0876af497fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different reasoning budgets for the complex problem\n",
    "print(\"Testing different reasoning budgets on the sock drawer problem...\")\n",
    "\n",
    "results = []\n",
    "budgets = [1024, 2048, 4096, 8192, 16384]  # Different budget sizes to test\n",
    "\n",
    "for budget in budgets:\n",
    "    print(f\"\\nTesting with reasoning budget: {budget} tokens\")\n",
    "    response = invoke_claude(\n",
    "        complex_prompt,\n",
    "        enable_reasoning=True,\n",
    "        reasoning_budget=budget,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    # Extract metrics\n",
    "    metrics = {\n",
    "        'budget': budget,\n",
    "        'time': response.get('_elapsed_time', 0),\n",
    "        'input_tokens': response.get('usage', {}).get('inputTokens', 0),\n",
    "        'output_tokens': response.get('usage', {}).get('outputTokens', 0),\n",
    "        'total_tokens': response.get('usage', {}).get('totalTokens', 0),\n",
    "        'cost': (response.get('usage', {}).get('inputTokens', 0) * 0.000003) + \n",
    "                (response.get('usage', {}).get('outputTokens', 0) * 0.000015)\n",
    "    }\n",
    "    \n",
    "    results.append(metrics)\n",
    "    \n",
    "    # Display brief summary\n",
    "    print(f\"Time: {metrics['time']:.2f}s, Tokens: {metrics['total_tokens']}, Cost: ${metrics['cost']:.5f}\")\n",
    "\n",
    "# Create a DataFrame and display the results\n",
    "performance_df = pd.DataFrame(results)\n",
    "display(performance_df)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(performance_df['budget'], performance_df['time'], marker='o')\n",
    "plt.title('Time vs. Reasoning Budget')\n",
    "plt.xlabel('Reasoning Budget (tokens)')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(performance_df['budget'], performance_df['total_tokens'], marker='o')\n",
    "plt.title('Total Tokens vs. Reasoning Budget')\n",
    "plt.xlabel('Reasoning Budget (tokens)')\n",
    "plt.ylabel('Total Tokens')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(performance_df['budget'], performance_df['cost'], marker='o')\n",
    "plt.title('Cost vs. Reasoning Budget')\n",
    "plt.xlabel('Reasoning Budget (tokens)')\n",
    "plt.ylabel('Cost ($)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "efficiency = performance_df['total_tokens'] / performance_df['time']\n",
    "plt.plot(performance_df['budget'], efficiency, marker='o')\n",
    "plt.title('Efficiency (Tokens/Second) vs. Reasoning Budget')\n",
    "plt.xlabel('Reasoning Budget (tokens)')\n",
    "plt.ylabel('Tokens per Second')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110bcb4-2d13-459d-a1ff-ba9874949bbf",
   "metadata": {},
   "source": [
    "## 6. When to Use Extended Thinking\n",
    "\n",
    "Based on our experiments and performance analysis, we can draw some conclusions about when to use extended thinking:\n",
    "\n",
    "### Best use cases for extended thinking:\n",
    "\n",
    "1. **Complex reasoning tasks**: Math problems, logic puzzles, and multi-step reasoning benefit significantly from extended thinking.\n",
    "\n",
    "2. **Problems requiring exhaustive analysis**: When Claude needs to consider many possibilities or edge cases.\n",
    "\n",
    "3. **When accuracy is critical**: Extended thinking generally improves accuracy on challenging problems by reducing the chance of reasoning errors.\n",
    "\n",
    "4. **Transparency requirements**: When you need to see Claude's reasoning process to verify its approach.\n",
    "\n",
    "### When standard mode might be sufficient:\n",
    "\n",
    "1. **Simple factual queries**: For straightforward questions like \"What is the capital of France?\", extended thinking adds cost without significant benefit.\n",
    "\n",
    "2. **Creative tasks**: Creative writing, summarization, and other content generation tasks may not benefit as much from extended thinking.\n",
    "\n",
    "3. **Time-sensitive applications**: If response speed is critical, standard mode provides faster responses.\n",
    "\n",
    "4. **Cost-sensitive applications**: Extended thinking increases token usage and therefore cost.\n",
    "\n",
    "### Finding the right reasoning budget:\n",
    "\n",
    "The ideal reasoning budget depends on the complexity of your task:\n",
    "\n",
    "- **Simple reasoning tasks**: 1,024-2,048 tokens\n",
    "- **Moderate complexity**: 2,048-4,096 tokens\n",
    "- **Complex problems**: 4,096-8,192 tokens\n",
    "- **Very complex problems**: 8,192+ tokens\n",
    "\n",
    "As we observed in our performance analysis, there's often a \"sweet spot\" where:\n",
    "- Too small a budget may not give Claude enough space to solve complex problems\n",
    "- Beyond a certain point, larger budgets show diminishing returns while increasing cost and latency\n",
    "- The efficiency (tokens per second) tends to peak at moderate budget sizes before declining\n",
    "\n",
    "Our performance charts demonstrate this tradeoff visually, helping you determine the optimal budget for your specific use cases.\n",
    "\n",
    "In the next notebook, we'll explore a more systematic framework for determining when to use extended thinking and how to optimize the reasoning budget for different task types."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
