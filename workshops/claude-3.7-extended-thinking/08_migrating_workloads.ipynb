{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c344e831-873a-40ae-b206-30a54a43d036",
   "metadata": {},
   "source": [
    "# Migrating Workloads to Claude 3.7\n",
    "\n",
    "This notebook demonstrates how to effectively migrate existing prompts and workflows from Claude 3.5 Sonnet to Claude 3.7, with a focus on leveraging the new extended thinking capability. You'll learn:\n",
    "\n",
    "1. How to identify prompts that would benefit from extended thinking\n",
    "2. Techniques for refactoring chain-of-thought prompts\n",
    "3. Best practices for prompt simplification\n",
    "4. Before/after examples with performance comparisons\n",
    "\n",
    "## Prerequisites\n",
    "- Understanding of Claude 3.5 Sonnet's capabilities\n",
    "- Familiarity with chain-of-thought prompting\n",
    "- Knowledge of Python and the Bedrock API\n",
    "- Completion of previous lessons (especially Lessons 1-2 on extended thinking)\n",
    "\n",
    "Let's begin by setting up our environment and importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f93836-b22b-410e-9742-8026b022956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Import our utility functions from previous lessons\n",
    "import claude_utils\n",
    "\n",
    "# Set up the Bedrock clients using our utility module\n",
    "REGION = 'us-west-2'  # Change to your preferred region\n",
    "bedrock, bedrock_runtime = claude_utils.create_bedrock_clients(REGION)\n",
    "\n",
    "# Define model IDs for comparison\n",
    "CLAUDE_35_SONNET_MODEL_ID = 'us.anthropic.claude-3-5-sonnet-20241022-v2:0'\n",
    "CLAUDE_37_SONNET_MODEL_ID = 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'\n",
    "\n",
    "# Verify model availability\n",
    "claude_utils.verify_model_availability(bedrock, CLAUDE_37_SONNET_MODEL_ID)\n",
    "claude_utils.verify_model_availability(bedrock, CLAUDE_35_SONNET_MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b6d90-1126-4557-b4cb-02984feb5aa9",
   "metadata": {},
   "source": [
    "## Understanding When to Migrate\n",
    "\n",
    "Before diving into specific migration techniques, it's important to understand which prompts are good candidates for migration to Claude 3.7's extended thinking capability.\n",
    "\n",
    "### Good Candidates for Migration:\n",
    "- Prompts that use explicit chain-of-thought instructions\n",
    "- Complex reasoning tasks requiring >1024 tokens of thinking\n",
    "- Multi-step problem solving workflows\n",
    "- Tasks that benefit from methodical analysis\n",
    "\n",
    "### Less Suitable for Migration:\n",
    "- Simple factual queries\n",
    "- Basic content generation\n",
    "- Tasks that don't require detailed reasoning\n",
    "- Workflows that already work well with standard prompting\n",
    "\n",
    "Let's examine some concrete examples to understand these differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51539ca-78e8-4296-8a47-b67261b8acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(prompt, chain_of_thought=True, reasoning_budget=4096):\n",
    "    \"\"\"\n",
    "    Compare responses between Claude 3.5 with chain-of-thought and Claude 3.7 with extended thinking\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The base prompt to test\n",
    "        chain_of_thought (bool): Whether to add chain-of-thought instructions for 3.5\n",
    "        reasoning_budget (int): Token budget for 3.7's extended thinking\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Claude 3.5 response, Claude 3.7 response)\n",
    "    \"\"\"\n",
    "    # Create 3.5 prompt with optional chain-of-thought\n",
    "    if chain_of_thought:\n",
    "        cot_prompt = f\"\"\"\n",
    "        Let's solve this step by step:\n",
    "        1. First, carefully analyze what's being asked\n",
    "        2. Break down the problem into parts\n",
    "        3. Solve each part methodically\n",
    "        4. Combine the results for a final answer\n",
    "\n",
    "        {prompt}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        cot_prompt = prompt\n",
    "    \n",
    "    # Get 3.5 response\n",
    "    response_35 = claude_utils.invoke_claude(\n",
    "        bedrock_runtime,\n",
    "        cot_prompt,\n",
    "        CLAUDE_35_SONNET_MODEL_ID,\n",
    "        enable_reasoning=False,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    # Get 3.7 response with extended thinking\n",
    "    response_37 = claude_utils.invoke_claude(\n",
    "        bedrock_runtime,\n",
    "        prompt,  # Note: No CoT instructions needed\n",
    "        CLAUDE_37_SONNET_MODEL_ID,\n",
    "        enable_reasoning=True,\n",
    "        reasoning_budget=reasoning_budget,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    return response_35, response_37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b790e0-2f4b-4fd0-becd-47ce3a43912f",
   "metadata": {},
   "source": [
    "## Example 1: Mathematical Problem Solving\n",
    "\n",
    "Let's start with a classic example: a mathematical word problem that traditionally used chain-of-thought prompting. This type of problem is an excellent candidate for migration because it:\n",
    "- Requires structured reasoning\n",
    "- Benefits from showing intermediate steps\n",
    "- Often involves multiple calculations\n",
    "- Needs validation of intermediate results\n",
    "\n",
    "We'll compare how this problem was handled in Claude 3.5 versus how it can be solved more elegantly with Claude 3.7's extended thinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cce7d7b-5ead-409f-9224-c080810ae786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional math word problem\n",
    "math_problem = \"\"\"\n",
    "A store is having a 30% off sale. A customer buys three items:\n",
    "- A jacket originally priced at $120\n",
    "- A pair of shoes originally priced at $85\n",
    "- A shirt originally priced at $45\n",
    "\n",
    "If there is an additional 10% discount for spending over $200 (calculated after the 30% sale discount),\n",
    "how much does the customer save in total, and what is their final cost?\n",
    "\n",
    "Show all calculations clearly.\n",
    "\"\"\"\n",
    "\n",
    "# Compare the models' responses\n",
    "response_35, response_37 = compare_models(\n",
    "    math_problem,\n",
    "    chain_of_thought=True,  # Enable CoT for 3.5\n",
    "    reasoning_budget=2048   # Moderate budget for this problem\n",
    ")\n",
    "\n",
    "# Display responses for comparison\n",
    "print(\"Claude 3.5 Response (with chain-of-thought):\")\n",
    "print(\"-\" * 80)\n",
    "claude_utils.display_claude_response(response_35)\n",
    "\n",
    "print(\"\\nClaude 3.7 Response (with extended thinking):\")\n",
    "print(\"-\" * 80)\n",
    "claude_utils.display_claude_response(response_37)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9830f2a0-8ad7-4591-8c4d-7254d65b503e",
   "metadata": {},
   "source": [
    "### Key Differences in the Responses\n",
    "\n",
    "Let's analyze how the two models approached this problem:\n",
    "\n",
    "1. **Prompt Structure**\n",
    "   - Claude 3.5 needed explicit step-by-step instructions\n",
    "   - Claude 3.7 developed its own reasoning approach\n",
    "\n",
    "2. **Response Quality**\n",
    "   - Completeness of calculations\n",
    "   - Clarity of explanation\n",
    "   - Validation of results\n",
    "\n",
    "3. **Token Efficiency**\n",
    "   - Compare prompt lengths\n",
    "   - Compare response lengths\n",
    "   - Overall token usage\n",
    "\n",
    "These differences highlight why extended thinking can be more effective than traditional chain-of-thought prompting for mathematical reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0686ead-c737-4407-a68e-502c7cb0f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comparison_metrics(response_35, response_37):\n",
    "    \"\"\"\n",
    "    Display detailed comparison metrics between the two responses\n",
    "    \"\"\"\n",
    "    # Calculate metrics for 3.5\n",
    "    tokens_35 = response_35.get('usage', {}).get('totalTokens', 0)\n",
    "    time_35 = response_35.get('_elapsed_time', 0)\n",
    "    \n",
    "    # Calculate metrics for 3.7\n",
    "    tokens_37 = response_37.get('usage', {}).get('totalTokens', 0)\n",
    "    time_37 = response_37.get('_elapsed_time', 0)\n",
    "    \n",
    "    # Display comparison\n",
    "    print(\"Performance Comparison:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Claude 3.5 (with CoT):\")\n",
    "    print(f\"- Total tokens: {tokens_35:,}\")\n",
    "    print(f\"- Response time: {time_35:.2f} seconds\")\n",
    "    print(f\"- Tokens per second: {tokens_35/time_35:.1f}\")\n",
    "    print(\"\\nClaude 3.7 (with extended thinking):\")\n",
    "    print(f\"- Total tokens: {tokens_37:,}\")\n",
    "    print(f\"- Response time: {time_37:.2f} seconds\")\n",
    "    print(f\"- Tokens per second: {tokens_37/time_37:.1f}\")\n",
    "    \n",
    "    # Calculate efficiency gains/losses\n",
    "    token_diff = ((tokens_37 - tokens_35) / tokens_35) * 100\n",
    "    time_diff = ((time_37 - time_35) / time_35) * 100\n",
    "    \n",
    "    print(\"\\nEfficiency Comparison:\")\n",
    "    print(f\"Token usage change: {token_diff:+.1f}%\")\n",
    "    print(f\"Response time change: {time_diff:+.1f}%\")\n",
    "\n",
    "# Display metrics for our math problem example\n",
    "display_comparison_metrics(response_35, response_37)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ffd1dc-4f1d-4c2c-a767-f5168e31cff2",
   "metadata": {},
   "source": [
    "## Example 2: Analysis and Recommendations\n",
    "\n",
    "For our second example, let's look at a more complex analytical task that requires synthesizing information and making recommendations. This type of prompt commonly used chain-of-thought to:\n",
    "- Structure the analysis process\n",
    "- Ensure consideration of multiple factors\n",
    "- Guide the development of recommendations\n",
    "- Maintain logical flow\n",
    "\n",
    "This example will demonstrate how Claude 3.7's extended thinking can handle complex analysis more naturally than prescribed chain-of-thought steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857c2c8-ce61-4113-bb39-e744d263d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex analysis problem\n",
    "analysis_prompt = \"\"\"\n",
    "Analyze the potential impact of implementing a four-day work week at a software company with 500 employees.\n",
    "\n",
    "Consider:\n",
    "- Employee productivity and satisfaction\n",
    "- Project timelines and deadlines\n",
    "- Customer support availability\n",
    "- Operating costs and profitability\n",
    "- Team coordination and meetings\n",
    "- Industry competitiveness\n",
    "\n",
    "Provide specific recommendations for successfully implementing this change.\n",
    "\"\"\"\n",
    "\n",
    "# Compare the models' responses\n",
    "response_35, response_37 = compare_models(\n",
    "    analysis_prompt,\n",
    "    chain_of_thought=True,     # Enable CoT for 3.5\n",
    "    reasoning_budget=4096      # Larger budget for complex analysis\n",
    ")\n",
    "\n",
    "# Display responses for comparison\n",
    "print(\"Claude 3.5 Response (with chain-of-thought):\")\n",
    "print(\"-\" * 80)\n",
    "claude_utils.display_claude_response(response_35)\n",
    "\n",
    "print(\"\\nClaude 3.7 Response (with extended thinking):\")\n",
    "print(\"-\" * 80)\n",
    "claude_utils.display_claude_response(response_37)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d39d9-e175-489f-be3b-41992dbd14bc",
   "metadata": {},
   "source": [
    "### Migration Pattern: From Structured to Natural Reasoning\n",
    "\n",
    "This example reveals a key pattern in migrating from Claude 3.5 to 3.7:\n",
    "\n",
    "1. **Traditional Chain-of-Thought Pattern**\n",
    "   - Required explicit analysis steps\n",
    "   - Needed structural guidance\n",
    "   - Often resulted in formulaic responses\n",
    "\n",
    "2. **Extended Thinking Pattern**\n",
    "   - Develops natural reasoning flow\n",
    "   - Integrates multiple perspectives organically\n",
    "   - Produces more nuanced analysis\n",
    "\n",
    "### Key Benefits of Migration\n",
    "- More comprehensive analysis\n",
    "- Better integration of related factors\n",
    "- More natural flow of ideas\n",
    "- Stronger, more contextual recommendations\n",
    "\n",
    "This pattern is particularly valuable for complex analytical tasks where rigid structure might limit insight development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697205a-9b6e-4f27-8d26-afdc1d59ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_response_patterns(response_35, response_37):\n",
    "    \"\"\"\n",
    "    Analyze response patterns between Claude 3.5 and 3.7\n",
    "    \n",
    "    Args:\n",
    "        response_35 (dict): Response from Claude 3.5\n",
    "        response_37 (dict): Response from Claude 3.7\n",
    "        \n",
    "    Returns:\n",
    "        None: Prints analysis to stdout\n",
    "    \"\"\"\n",
    "    # Safely extract responses with error handling\n",
    "    text_35 = claude_utils.extract_response_content(response_35) or \"\"\n",
    "    text_37 = claude_utils.extract_response_content(response_37) or \"\"\n",
    "    \n",
    "    if not text_35 or not text_37:\n",
    "        print(\"Error: One or both responses are empty\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Analyze structure\n",
    "        sections_35 = len([l for l in text_35.split('\\n') if l.strip().startswith('#')])\n",
    "        sections_37 = len([l for l in text_37.split('\\n') if l.strip().startswith('#')])\n",
    "        \n",
    "        bullets_35 = len([l for l in text_35.split('\\n') if l.strip().startswith('-')])\n",
    "        bullets_37 = len([l for l in text_37.split('\\n') if l.strip().startswith('-')])\n",
    "        \n",
    "        # Calculate paragraphs safely\n",
    "        paragraphs_35 = len([p for p in text_35.split('\\n\\n') if p.strip()])\n",
    "        paragraphs_37 = len([p for p in text_37.split('\\n\\n') if p.strip()])\n",
    "        \n",
    "        # Calculate words safely\n",
    "        words_35 = len(text_35.split())\n",
    "        words_37 = len(text_37.split())\n",
    "        \n",
    "        # Display analysis\n",
    "        print(\"Response Pattern Analysis:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"Claude 3.5 Structure:\")\n",
    "        print(f\"- Main sections: {sections_35}\")\n",
    "        print(f\"- Bullet points: {bullets_35}\")\n",
    "        print(f\"- Average words per paragraph: {words_35/max(1, paragraphs_35):.1f}\")\n",
    "        print(f\"- Claude 3.5 v2 Response: \\n\\n{text_35}\")\n",
    "        \n",
    "        print(\"\\nClaude 3.7 Structure:\")\n",
    "        print(f\"- Main sections: {sections_37}\")\n",
    "        print(f\"- Bullet points: {bullets_37}\")\n",
    "        print(f\"- Average words per paragraph: {words_37/max(1, paragraphs_37):.1f}\")\n",
    "        print(f\"- Claude 3.7 Response: \\n\\n{text_37}\")\n",
    "        \n",
    "        # Analyze differences in approach\n",
    "        print(\"\\nKey Differences:\")\n",
    "        print(f\"- Section organization: {'More' if sections_37 > sections_35 else 'Less'} structured\")\n",
    "        print(f\"- Point presentation: {'More' if bullets_37 > bullets_35 else 'Less'} bullet points\")\n",
    "        print(f\"- Writing style: {'More' if words_37 > words_35 else 'Less'} detailed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing responses: {str(e)}\")\n",
    "\n",
    "# Analyze patterns in our analysis example\n",
    "analyze_response_patterns(response_35, response_37)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9375f-87dd-4d41-8b32-48968d7bb355",
   "metadata": {},
   "source": [
    "## Best Practices for Prompt Migration\n",
    "\n",
    "When migrating prompts from Claude 3.5 to Claude 3.7, several key practices help ensure successful transitions:\n",
    "\n",
    "### 1. Remove Unnecessary Structure\n",
    "- Eliminate explicit step-by-step instructions\n",
    "- Remove artificial thinking markers\n",
    "- Let extended thinking develop natural flow\n",
    "\n",
    "### 2. Adjust Token Budgets\n",
    "- Start with minimum 1024 tokens for extended thinking\n",
    "- Scale budget based on task complexity\n",
    "- Monitor and optimize based on results\n",
    "\n",
    "### 3. Focus on Clear Requirements\n",
    "- State objectives clearly\n",
    "- Specify constraints directly\n",
    "- Provide relevant context\n",
    "\n",
    "Let's examine these practices with a systematic example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3e3e5-ee99-4b1d-9f81-c3c77a8f485c",
   "metadata": {},
   "source": [
    "#### Prompt Migration Helper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e59ef8-f5cb-438a-b150-359538c813e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_prompt_migration(original_prompt, verbose=True):\n",
    "    \"\"\"\n",
    "    Use Claude 3.7 to migrate a prompt from chain-of-thought style to extended thinking style\n",
    "    \n",
    "    Args:\n",
    "        original_prompt (str): Original chain-of-thought style prompt\n",
    "        verbose (bool): Whether to show detailed transformation steps\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (original prompt, migrated prompt)\n",
    "    \"\"\"\n",
    "    # Create a prompt for Claude to handle the migration\n",
    "    migration_request = f\"\"\"\n",
    "    Help me migrate this prompt from a chain-of-thought style (used with Claude 3.5) to a cleaner style for Claude 3.7's extended thinking.\n",
    "\n",
    "    Original prompt:\n",
    "    {original_prompt}\n",
    "\n",
    "    Guidelines for migration:\n",
    "    - Remove explicit step-by-step instructions\n",
    "    - Remove artificial thinking markers\n",
    "    - Remove any model-specific steering language for example, instructions about laziness or verbosity\n",
    "    - Remove chain-of-thought guidance and logic\n",
    "    - Preserve important context and requirements\n",
    "    - Make the prompt more natural and direct\n",
    "    - Keep the core question/request clear\n",
    "    \n",
    "    Please provide only the migrated prompt with no additional explanation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get Claude's migration suggestion\n",
    "    response = claude_utils.invoke_claude(\n",
    "        bedrock_runtime,\n",
    "        migration_request,\n",
    "        CLAUDE_37_SONNET_MODEL_ID,\n",
    "        enable_reasoning=True,\n",
    "        reasoning_budget=2048,  # Moderate budget for this task\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    # Extract the migrated prompt\n",
    "    migrated_prompt = claude_utils.extract_response_content(response)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Original Prompt:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(original_prompt)\n",
    "        print(\"\\nMigrated Prompt:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(migrated_prompt)\n",
    "        \n",
    "    return original_prompt, migrated_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0043a75-52f0-4f09-b831-0895978f83ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a typical chain-of-thought prompt\n",
    "original_prompt = \"\"\"\n",
    "Let's solve this complex optimization problem step by step:\n",
    "\n",
    "You are planning a conference with the following constraints:\n",
    "1. First, determine the venue capacity needed for 300 attendees\n",
    "2. Then, calculate the budget including:\n",
    "   - Venue rental ($5000/day)\n",
    "   - Catering ($75/person/day)\n",
    "   - Equipment rental ($2000/day)\n",
    "3. Next, optimize the schedule for:\n",
    "   - 3 parallel tracks\n",
    "   - 45-minute sessions\n",
    "   - 15-minute breaks\n",
    "4. Finally, recommend the optimal conference duration in days\n",
    "\n",
    "Think carefully about each step before providing recommendations.\n",
    "\"\"\"\n",
    "\n",
    "# Demonstrate migration\n",
    "orig, migrated = demonstrate_prompt_migration(original_prompt)\n",
    "\n",
    "# Test both versions\n",
    "print(\"\\nTesting both versions:\")\n",
    "response_35, response_37 = compare_models(\n",
    "    migrated,  # Use migrated prompt for both to compare approaches\n",
    "    chain_of_thought=True,  # 3.5 gets CoT instructions\n",
    "    reasoning_budget=4096   # 3.7 gets extended thinking\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"\\nResults Comparison:\")\n",
    "display_comparison_metrics(response_35, response_37)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f7c0af-435b-41a3-ac57-6788d51a2489",
   "metadata": {},
   "source": [
    "## Systematic Migration Patterns\n",
    "\n",
    "Based on our examples, we can identify several common patterns when migrating prompts from Claude 3.5 to Claude 3.7:\n",
    "\n",
    "### Pattern 1: Step-by-Step to Goal-Oriented\n",
    "- **Before**: Detailed step sequence with explicit instructions\n",
    "- **After**: Clear goal statement with relevant constraints and context\n",
    "\n",
    "### Pattern 2: Explicit to Implicit Reasoning\n",
    "- **Before**: \"Think through X, Y, Z considerations\"\n",
    "- **After**: \"Consider the following factors: X, Y, Z\"\n",
    "\n",
    "### Pattern 3: Structured Output to Natural Output\n",
    "- **Before**: \"Format your response with sections A, B, C\"\n",
    "- **After**: \"Include information about A, B, and C in your response\"\n",
    "\n",
    "Let's see these patterns in practice with more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2350b7d-dd45-4ec6-9dcd-25f721c8e086",
   "metadata": {},
   "source": [
    "## Check-in Point: Prompt Migration Principles\n",
    "\n",
    "Let's review what we've learned about migrating prompts:\n",
    "\n",
    "1. **Structural Changes**\n",
    "   - What types of instructional markers can be safely removed?\n",
    "   - How does prompt length change after migration?\n",
    "   - What elements should be preserved?\n",
    "\n",
    "2. **Performance Impact**\n",
    "   - How does token usage compare?\n",
    "   - What are the response time differences?\n",
    "   - Does response quality improve?\n",
    "\n",
    "3. **Best Practices**\n",
    "   - When should you retain some structure?\n",
    "   - How do you choose appropriate reasoning budgets?\n",
    "   - What makes a prompt migration successful?\n",
    "\n",
    "These insights will help guide your own prompt migration efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acacd23-fe54-42f0-848a-3f179556075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define examples of different migration patterns\n",
    "migration_examples = {\n",
    "    \"step_by_step\": {\n",
    "        \"before\": \"\"\"\n",
    "        Let's solve this math problem step by step:\n",
    "        1. First, calculate the total cost before discount\n",
    "        2. Then, apply the 15% discount\n",
    "        3. Next, add 8% sales tax\n",
    "        4. Finally, determine the final price\n",
    "        \n",
    "        How much would a $120 item cost after discount and tax?\n",
    "        \"\"\",\n",
    "        \"complexity\": \"simple\"\n",
    "    },\n",
    "    \n",
    "    \"explicit_reasoning\": {\n",
    "        \"before\": \"\"\"\n",
    "        Think through the implications of remote work by considering:\n",
    "        First, analyze productivity impacts.\n",
    "        Then, examine employee satisfaction effects.\n",
    "        Next, evaluate communication challenges.\n",
    "        Finally, weigh cost savings against potential drawbacks.\n",
    "        \n",
    "        Is remote work a net positive for most organizations?\n",
    "        \"\"\",\n",
    "        \"complexity\": \"medium\"\n",
    "    },\n",
    "    \n",
    "    \"structured_output\": {\n",
    "        \"before\": \"\"\"\n",
    "        Analyze this short story following these steps:\n",
    "        1. First, summarize the plot\n",
    "        2. Then, identify the main themes\n",
    "        3. Next, analyze the character development\n",
    "        4. Finally, discuss the author's writing style\n",
    "        \n",
    "        Structure your analysis with clear sections for each aspect.\n",
    "        \"\"\",\n",
    "        \"complexity\": \"complex\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Migrate and display each example\n",
    "for pattern_name, example in migration_examples.items():\n",
    "    print(f\"\\n\\n{'='*80}\\nMigration Pattern: {pattern_name.replace('_', ' ').title()}\")\n",
    "    print(f\"Complexity: {example['complexity'].title()}\")\n",
    "    \n",
    "    # Migrate the prompt\n",
    "    _, migrated = demonstrate_prompt_migration(example[\"before\"], verbose=False)\n",
    "    \n",
    "    # Display before/after\n",
    "    print(\"\\nBefore:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(example[\"before\"])\n",
    "    \n",
    "    print(\"\\nAfter:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(migrated)\n",
    "    \n",
    "    # Determine appropriate budget based on complexity\n",
    "    budget_map = {\"simple\": 1024, \"medium\": 2048, \"complex\": 4096}\n",
    "    budget = budget_map.get(example[\"complexity\"], 2048)\n",
    "    \n",
    "    print(f\"\\nRecommended reasoning budget: {budget} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b40c32-6f31-4645-9714-d3d2fbcdc97f",
   "metadata": {},
   "source": [
    "## Practical Migration Workflow\n",
    "\n",
    "For systematically migrating your existing prompts to Claude 3.7, follow this practical workflow:\n",
    "\n",
    "### 1. Inventory and Prioritize\n",
    "- List all your existing prompts\n",
    "- Identify those with explicit chain-of-thought instructions\n",
    "- Prioritize complex reasoning tasks that would benefit most\n",
    "\n",
    "### 2. Analyze Current Prompt\n",
    "- Identify reasoning instructions\n",
    "- Note required context and constraints\n",
    "- Recognize output format requirements\n",
    "\n",
    "### 3. Simplify and Migrate\n",
    "- Remove explicit reasoning steps\n",
    "- Preserve critical context and requirements\n",
    "- Consider using Claude 3.7 itself to help with migration\n",
    "\n",
    "### 4. Test and Refine\n",
    "- Start with minimum reasoning budget (1024 tokens)\n",
    "- Increase budget if needed based on task complexity\n",
    "- Compare results and adjust as needed\n",
    "\n",
    "This systematic approach ensures successful migration while optimizing for Claude 3.7's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc95e92-b2d6-4fc5-b28f-3e1d36fa612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_migration_checklist(prompt):\n",
    "    \"\"\"\n",
    "    Generate a migration checklist for a given prompt\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The original prompt to analyze\n",
    "        \n",
    "    Returns:\n",
    "        dict: Migration recommendations\n",
    "    \"\"\"\n",
    "    analysis_request = f\"\"\"\n",
    "    Analyze this prompt that was designed for Claude 3.5 with chain-of-thought instructions.\n",
    "    \n",
    "    Prompt:\n",
    "    {prompt}\n",
    "    \n",
    "    Please create a migration checklist with the following information in JSON format:\n",
    "    1. Elements to remove (e.g., step-by-step instructions, thinking markers)\n",
    "    2. Elements to preserve (e.g., key context, important constraints)\n",
    "    3. Recommended reasoning budget (1024, 2048, 4096, or 8192 tokens)\n",
    "    4. Complexity assessment (simple, medium, complex, very complex)\n",
    "    \n",
    "    Respond with only the JSON data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get Claude's analysis\n",
    "    response = claude_utils.invoke_claude(\n",
    "        bedrock_runtime,\n",
    "        analysis_request,\n",
    "        CLAUDE_37_SONNET_MODEL_ID,\n",
    "        enable_reasoning=True,\n",
    "        reasoning_budget=2048,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    # Extract and parse JSON\n",
    "    checklist_text = claude_utils.extract_response_content(response)\n",
    "    \n",
    "    try:\n",
    "        # Try to parse as JSON\n",
    "        import json\n",
    "        checklist = json.loads(checklist_text)\n",
    "        \n",
    "        # Display in a readable format\n",
    "        print(\"Migration Checklist:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"Elements to Remove:\")\n",
    "        for item in checklist.get(\"Elements to remove\", []):\n",
    "            print(f\"- {item}\")\n",
    "            \n",
    "        print(\"\\nElements to Preserve:\")\n",
    "        for item in checklist.get(\"Elements to preserve\", []):\n",
    "            print(f\"- {item}\")\n",
    "            \n",
    "        print(f\"\\nComplexity Assessment: {checklist.get('Complexity assessment', 'Unknown')}\")\n",
    "        print(f\"Recommended Reasoning Budget: {checklist.get('Recommended reasoning budget', 1024)} tokens\")\n",
    "        \n",
    "        return checklist\n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error parsing response as JSON. Raw response:\")\n",
    "        print(checklist_text)\n",
    "        return None\n",
    "\n",
    "# Test the migration checklist function\n",
    "complex_prompt = \"\"\"\n",
    "Let's analyze this company's financial performance step by step:\n",
    "\n",
    "1. First, calculate the year-over-year revenue growth rate\n",
    "2. Then, analyze the profit margins (gross, operating, net)\n",
    "3. Next, evaluate the debt-to-equity ratio and liquidity metrics\n",
    "4. Then, compare performance against industry benchmarks\n",
    "5. Finally, identify key strengths and areas for improvement\n",
    "\n",
    "Provide a structured analysis with clear sections for each aspect.\n",
    "\"\"\"\n",
    "\n",
    "migration_checklist = create_migration_checklist(complex_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53de523-e52a-484c-9867-6958393d2cd9",
   "metadata": {},
   "source": [
    "## Handling Complex Migration Scenarios\n",
    "\n",
    "While many prompts can be easily migrated following our patterns, certain scenarios require special consideration:\n",
    "\n",
    "### Multi-Stage Reasoning Workflows\n",
    "\n",
    "For complex workflows that previously relied on a sequence of reasoning steps, the migration strategy needs to:\n",
    "- Preserve critical dependencies between steps\n",
    "- Maintain clarity about the overall goal\n",
    "- Ensure all necessary context is included\n",
    "\n",
    "### Domain-Specific Requirements\n",
    "\n",
    "Some domains (like mathematical proofs, scientific analysis, or code generation) may have specific expectations about:\n",
    "- Notation and formatting\n",
    "- Methodology and approach\n",
    "- Verification and validation\n",
    "\n",
    "Let's examine how to handle these more complex migrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c8527-2967-49d6-9e5a-7937a626303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a multi-stage reasoning workflow\n",
    "multistage_prompt = \"\"\"\n",
    "Let's solve this complex data analysis problem step by step:\n",
    "\n",
    "1. First, analyze the customer segmentation data:\n",
    "   - Identify primary customer segments\n",
    "   - Calculate average revenue per segment\n",
    "   - Determine growth rate for each segment\n",
    "\n",
    "2. Then, evaluate marketing channel effectiveness:\n",
    "   - Compare customer acquisition cost by channel\n",
    "   - Calculate ROI for each marketing channel\n",
    "   - Identify the most and least effective channels\n",
    "\n",
    "3. Next, forecast future performance:\n",
    "   - Project segment growth for next 12 months\n",
    "   - Estimate marketing budget requirements\n",
    "   - Predict overall revenue impact\n",
    "\n",
    "4. Finally, prioritize recommendations:\n",
    "   - Rank segments by potential value\n",
    "   - Suggest budget reallocation between channels\n",
    "   - Propose specific strategies for high-value segments\n",
    "\n",
    "Ensure your analysis follows this structure and provides clear recommendations.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Complex Multi-Stage Workflow Example:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Migrate the prompt\n",
    "_, migrated_multistage = demonstrate_prompt_migration(multistage_prompt, verbose=False)\n",
    "\n",
    "# Show migration results\n",
    "print(\"\\nOriginal Multi-Stage Prompt:\")\n",
    "print(\"-\" * 40)\n",
    "print(multistage_prompt)\n",
    "\n",
    "print(\"\\nMigrated Multi-Stage Prompt:\")\n",
    "print(\"-\" * 40)\n",
    "print(migrated_multistage)\n",
    "\n",
    "# Generate and display migration checklist\n",
    "print(\"\\nMigration Analysis:\")\n",
    "migration_checklist = create_migration_checklist(multistage_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb429c-eb90-4940-b641-e8f9769cd27a",
   "metadata": {},
   "source": [
    "## Before and After: A Complete Picture\n",
    "\n",
    "Let's summarize what we've learned by comparing a complete workflow before and after migration:\n",
    "\n",
    "### Claude 3.5 Approach:\n",
    "1. Detailed chain-of-thought instructions\n",
    "2. Explicit step sequencing\n",
    "3. Structured reasoning guidance\n",
    "4. Visible thinking process in prompt\n",
    "\n",
    "### Claude 3.7 Approach:\n",
    "1. Clear goal statement\n",
    "2. Relevant context and constraints\n",
    "3. Extended thinking (handled internally)\n",
    "4. Appropriate reasoning budget\n",
    "\n",
    "The key shift is from prescriptive reasoning (telling Claude exactly how to think) to goal-oriented reasoning (telling Claude what you want to achieve and letting its extended thinking handle the process).\n",
    "\n",
    "Let's visualize this difference with a side-by-side comparison of a complete workflow.\n",
    "![Complete Workflow](./images/lesson8/compare.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1e3d0f-c647-44af-a50e-e0a8b46862b1",
   "metadata": {},
   "source": [
    "## Performance Benchmarking Before and After Migration\n",
    "\n",
    "To truly validate the benefits of migrating to Claude 3.7's extended thinking, we should systematically benchmark performance. Let's create a framework for comparing:\n",
    "\n",
    "1. **Reasoning Quality**: How thorough and accurate is the reasoning?\n",
    "2. **Response Efficiency**: How efficiently are responses generated?\n",
    "3. **Token Usage**: How does total token consumption compare?\n",
    "\n",
    "By benchmarking these metrics across different types of prompts, we can make data-driven migration decisions and understand the real-world impact of the migration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5968edd9-d13b-46fc-81b1-60daf44b00d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_migration(prompts, reasoning_budgets=None):\n",
    "    \"\"\"\n",
    "    Benchmark performance before and after migration\n",
    "    \n",
    "    Args:\n",
    "        prompts (dict): Dictionary of prompts to benchmark\n",
    "        reasoning_budgets (dict, optional): Dictionary mapping prompt names to reasoning budgets\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Benchmark results\n",
    "    \"\"\"\n",
    "    # Default reasoning budgets if not provided\n",
    "    if reasoning_budgets is None:\n",
    "        reasoning_budgets = {\n",
    "            name: 4096 for name in prompts.keys()\n",
    "        }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, original_prompt in prompts.items():\n",
    "        print(f\"\\nBenchmarking: {name}\")\n",
    "        \n",
    "        # 1. Migrate the prompt\n",
    "        _, migrated_prompt = demonstrate_prompt_migration(original_prompt, verbose=False)\n",
    "        \n",
    "        # 2. Test Claude 3.5 with original prompt\n",
    "        start_time = time.time()\n",
    "        response_35 = claude_utils.invoke_claude(\n",
    "            bedrock_runtime,\n",
    "            original_prompt,\n",
    "            CLAUDE_35_SONNET_MODEL_ID,\n",
    "            enable_reasoning=False,\n",
    "            max_tokens=1500\n",
    "        )\n",
    "        time_35 = time.time() - start_time\n",
    "        \n",
    "        # 3. Test Claude 3.7 with migrated prompt\n",
    "        start_time = time.time()\n",
    "        response_37 = claude_utils.invoke_claude(\n",
    "            bedrock_runtime,\n",
    "            migrated_prompt,\n",
    "            CLAUDE_37_SONNET_MODEL_ID,\n",
    "            enable_reasoning=True,\n",
    "            reasoning_budget=reasoning_budgets.get(name, 4096),\n",
    "            max_tokens=1500\n",
    "        )\n",
    "        time_37 = time.time() - start_time\n",
    "        \n",
    "        # 4. Calculate metrics\n",
    "        tokens_35 = response_35.get('usage', {}).get('totalTokens', 0)\n",
    "        tokens_37 = response_37.get('usage', {}).get('totalTokens', 0)\n",
    "        \n",
    "        # 5. Extract response text to measure length\n",
    "        text_35 = claude_utils.extract_response_content(response_35) or \"\"\n",
    "        text_37 = claude_utils.extract_response_content(response_37) or \"\"\n",
    "        \n",
    "        # 6. Store results\n",
    "        results.append({\n",
    "            'Prompt': name,\n",
    "            'Original Length': len(original_prompt),\n",
    "            'Migrated Length': len(migrated_prompt),\n",
    "            'Prompt Change %': round(((len(migrated_prompt) - len(original_prompt)) / len(original_prompt) * 100), 1),\n",
    "            'Claude 3.5 Tokens': tokens_35,\n",
    "            'Claude 3.7 Tokens': tokens_37,\n",
    "            'Token Change %': round(((tokens_37 - tokens_35) / tokens_35 * 100), 1),\n",
    "            'Claude 3.5 Time (s)': round(time_35, 2),\n",
    "            'Claude 3.7 Time (s)': round(time_37, 2),\n",
    "            'Time Change %': round(((time_37 - time_35) / time_35 * 100), 1),\n",
    "            'Claude 3.5 Output Words': len(text_35.split()),\n",
    "            'Claude 3.7 Output Words': len(text_37.split()),\n",
    "            'Output Change %': round(((len(text_37.split()) - len(text_35.split())) / max(1, len(text_35.split())) * 100), 1),\n",
    "            'Reasoning Budget': reasoning_budgets.get(name, 4096)\n",
    "        })\n",
    "        \n",
    "        print(f\"  Original prompt: {len(original_prompt)} chars\")\n",
    "        print(f\"  Migrated prompt: {len(migrated_prompt)} chars\")\n",
    "        print(f\"  Claude 3.5: {tokens_35} tokens in {time_35:.2f}s\")\n",
    "        print(f\"  Claude 3.7: {tokens_37} tokens in {time_37:.2f}s\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "# Define prompts for benchmarking\n",
    "benchmark_prompts = {\n",
    "    \"Math Problem\": \"\"\"\n",
    "    Let's solve this probability problem step by step:\n",
    "    1. First, understand what we're looking for\n",
    "    2. Then, identify the probability of each event\n",
    "    3. Next, apply the appropriate probability formula\n",
    "    4. Finally, calculate the answer\n",
    "    \n",
    "    In a standard deck of 52 cards, what is the probability of drawing a face card (J, Q, K) or an ace?\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Business Analysis\": \"\"\"\n",
    "    Let's analyze this business case step by step:\n",
    "    1. First, identify the key issues\n",
    "    2. Then, analyze the market conditions\n",
    "    3. Next, evaluate the financial implications\n",
    "    4. Finally, recommend a course of action\n",
    "    \n",
    "    A retail company is considering expanding to online sales. They have $500K to invest and want to know if they should build their own platform or use an existing marketplace like Amazon.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Code Review\": \"\"\"\n",
    "    Let's review this code step by step:\n",
    "    1. First, understand what the code is trying to do\n",
    "    2. Then, check for logic errors\n",
    "    3. Next, identify performance issues\n",
    "    4. Finally, suggest improvements\n",
    "    \n",
    "    ```python\n",
    "    def find_duplicates(arr):\n",
    "        duplicates = []\n",
    "        for i in range(len(arr)):\n",
    "            for j in range(i+1, len(arr)):\n",
    "                if arr[i] == arr[j] and arr[i] not in duplicates:\n",
    "                    duplicates.append(arr[i])\n",
    "        return duplicates\n",
    "    ```\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Define reasoning budgets for each prompt\n",
    "reasoning_budgets = {\n",
    "    \"Math Problem\": 2048,\n",
    "    \"Business Analysis\": 4096,\n",
    "    \"Code Review\": 4096\n",
    "}\n",
    "\n",
    "# Run the benchmark\n",
    "benchmark_df = benchmark_migration(benchmark_prompts, reasoning_budgets)\n",
    "\n",
    "# Display the results\n",
    "display(HTML(\"<h3>Migration Benchmark Results</h3>\"))\n",
    "display(benchmark_df[['Prompt', 'Prompt Change %', 'Token Change %', 'Time Change %', 'Output Change %', 'Reasoning Budget']])\n",
    "\n",
    "# Create visualizations\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Plot token and time changes\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Token changes\n",
    "    benchmark_df.plot(\n",
    "        x='Prompt', \n",
    "        y=['Claude 3.5 Tokens', 'Claude 3.7 Tokens'], \n",
    "        kind='bar', \n",
    "        ax=ax[0],\n",
    "        color=['skyblue', 'lightgreen']\n",
    "    )\n",
    "    ax[0].set_title('Token Usage Comparison')\n",
    "    ax[0].set_ylabel('Tokens Used')\n",
    "    ax[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Time changes\n",
    "    benchmark_df.plot(\n",
    "        x='Prompt', \n",
    "        y=['Claude 3.5 Time (s)', 'Claude 3.7 Time (s)'], \n",
    "        kind='bar', \n",
    "        ax=ax[1],\n",
    "        color=['skyblue', 'lightgreen']\n",
    "    )\n",
    "    ax[1].set_title('Response Time Comparison')\n",
    "    ax[1].set_ylabel('Time (seconds)')\n",
    "    ax[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Visualization could not be created: {e}\")\n",
    "    print(\"Would display charts comparing token usage and response time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e4ee3e-55b7-4e37-8b1d-d300cc5e8e60",
   "metadata": {},
   "source": [
    "## When to Retain Chain-of-Thought Elements\n",
    "\n",
    "While we've focused on removing chain-of-thought elements when migrating to Claude 3.7, there are some situations where retaining certain aspects of structured prompting remains beneficial:\n",
    "\n",
    "### 1. Specific Output Format Requirements\n",
    "If you require a very specific output format, it can be helpful to specify this clearly. However, focus on the *what* (output format) rather than the *how* (thinking process).\n",
    "\n",
    "### 2. Domain-Specific Methodologies\n",
    "In specialized fields with established methodologies (scientific research, legal analysis, etc.), noting the required methodology can be valuable.\n",
    "\n",
    "### 3. Multiple Distinct Tasks\n",
    "When your prompt contains several unrelated tasks, structuring these as separate items can help organize the response (though not the reasoning process).\n",
    "\n",
    "### 4. Very Small Reasoning Budgets\n",
    "If you're using the minimum reasoning budget (1024 tokens) for a complex task, providing some high-level structure might help.\n",
    "\n",
    "The key principle is to guide *what* Claude should accomplish without prescribing *how* it should think through the problem - let extended thinking handle the reasoning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e38e6-6a0c-4563-b721-bb7bb32d573e",
   "metadata": {},
   "source": [
    "## Conclusion: Migration Strategy Checklist\n",
    "\n",
    "We've explored the process of migrating prompts from Claude 3.5 to Claude 3.7, focusing on leveraging the extended thinking capability for improved performance. Here's a checklist to guide your own migration efforts:\n",
    "\n",
    "1. **Audit Existing Prompts**\n",
    "   - Identify prompts with explicit reasoning instructions\n",
    "   - Prioritize complex reasoning tasks\n",
    "   - Note current token usage and performance\n",
    "\n",
    "2. **Simplify Prompts**\n",
    "   - Remove step-by-step instructions\n",
    "   - Eliminate explicit thinking markers\n",
    "   - Preserve essential context and requirements\n",
    "\n",
    "3. **Configure Extended Thinking**\n",
    "   - Start with minimum reasoning budget (1024 tokens)\n",
    "   - Scale budget based on task complexity\n",
    "   - Monitor response quality and adjust as needed\n",
    "\n",
    "4. **Validate Results**\n",
    "   - Compare response quality before and after\n",
    "   - Measure token usage and response times\n",
    "   - Make incremental improvements based on feedback\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Extended thinking enables more natural, flexible reasoning\n",
    "- Simpler prompts often lead to better results with Claude 3.7\n",
    "- Select reasoning budgets based on task complexity\n",
    "- Focus on what you want (goals) rather than how to get there (steps)\n",
    "\n",
    "By following these guidelines, you can successfully migrate your existing workloads to take full advantage of Claude 3.7's advanced capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
