{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Anthropic on AWS","text":""},{"location":"cookbooks/","title":"Cookbooks","text":""},{"location":"cookbooks/#bedrock-cookbooks","title":"bedrock-cookbooks","text":"<p>This repo contains cookbooks from anthropic-cookbook converted to use Amazon Bedrock instead of the Anthropic SDK. These cookbooks can be run using a platform that supports a Jupyter notebook like Amazon SageMaker.</p> <p>These notebooks were largely converted from the existing Anthropic examples using a Python script that uses Amazon Bedrock to perform code updates. This script can be found here.</p>"},{"location":"workshops/","title":"Workshops","text":"<ul> <li>Prompt Engineering with Anthropic Claude v3</li> </ul>"},{"location":"projects/classification-with-intercom/","title":"Classification with Intercom","text":""},{"location":"projects/classification-with-intercom/#intercom-ticket-classification-with-claude","title":"Intercom Ticket Classification with Claude","text":"<p>In this demo, we will see how we can integrate Intercom with Amazon Bedrock to provide ticket classification using the Anthropic Claude model. Using Webhooks from Intercom, when a new ticket is created an AWS Lambda function will be invoked that will use Amazon Bedrock to classify the ticket. The same Lambda function will then make an API request to Intercom to update the ticket with the new ticket assignment.</p>"},{"location":"projects/classification-with-intercom/#overview","title":"Overview","text":"<ol> <li>Ticket is created from the client</li> <li>Webhook notification is sent to Amazon API Gateway with ticket information</li> <li>Amazon API Gateway invokes AWS Lambda function</li> <li>Request is made to Amazon Bedrock</li> <li>Response is returned to AWS Lambda function</li> <li>API Request is made to Intercom to update ticket</li> </ol>"},{"location":"projects/classification-with-intercom/#set-up","title":"Set up","text":"<p>In order to use this demo, you'll need to set up your Intercom application for use with WebHooks. This is a high level overview of the process.</p> <ol> <li>Create a development workspace in Intercom</li> <li>Access the developer hub</li> <li>Create a new app</li> <li>Copy the access token from Authentication</li> <li>Create <code>.env</code> file and configure with <code>INTERCOM_API_TOKEN=</code> and the access token to the <code>.env</code> file</li> <li>Deploy the CDK</li> <li>Copy the <code>APIGatewayURL</code> from the CDK output</li> <li>Add the APIGatewayURL to the Webhook in Intercom</li> <li>Add relevant topics</li> <li>Create teams</li> <li>Copy App ID</li> <li>Create <code>intercom-client/.env</code> and configure with <code>VITE_INTERCOM_APP_ID=</code> App ID</li> <li>Start local client</li> </ol>"},{"location":"projects/classification-with-intercom/#intercom-configuration","title":"Intercom Configuration","text":"<p>To use this demo, you'll need to configure Intercom to send notification to the Amazon API Gateway. To do this, you will need to create an App in Intercom. This will be done through the developer hub. Once you have created the App, copy the access token generated. This will be used by the AWS Lambda function when making requests. We will update this using AWS Secrets Manager during the deployment.</p> <p></p>"},{"location":"projects/classification-with-intercom/#cdk-deployment","title":"CDK Deployment","text":"<p>This demo will deploy several resources within your AWS account. In order to deploy this CDK, you should have the following:</p> <ul> <li>Yarn installed</li> <li>AWS CLI installed and configured</li> <li>Claude Haiku access in Bedrock</li> </ul> <p>To deploy:</p> <p>Create a <code>.env</code> file and add your access token.</p> <pre><code>INTERCOM_API_TOKEN=\"xxxxxxxxxxx\"\n</code></pre> <p>Then, launch the CDK.</p> <pre><code>yarn launch\n</code></pre> <p>Once deployed, take note of the <code>IntercomIntegration.APIGatewayURL</code>. This will be used to configure the Webhooks within Intercom.</p>"},{"location":"projects/classification-with-intercom/#back-to-intercom","title":"Back to Intercom","text":""},{"location":"projects/classification-with-intercom/#webhook-configuration","title":"Webhook Configuration","text":"<p>In your app, add the URL to the Webhooks section. When configured with Topics, notifications will be sent to this target and consumed by our AWS Lambda function for processing.</p> <p></p> <p>Add the <code>ticket.created</code> topic to this Webhook. Other topics can be added for additional processing.</p> <p>To secure the Amazon API Gateway, we will restrict access to the API Gateway to only the Intercom IP addresses in the CDK deployment.</p> <pre><code>      policy: new PolicyDocument({\n        statements: [\n          new PolicyStatement({\n            principals: [new AnyPrincipal()],\n            actions: ['execute-api:Invoke'],\n            resources: ['execute-api:/*/*/*'],\n            effect: Effect.ALLOW,\n          }),\n          new PolicyStatement({\n            principals: [new AnyPrincipal()],\n            actions: ['execute-api:Invoke'],\n            resources: ['execute-api:/*/*/*'],\n            conditions: {\n              NotIpAddress: {\n                'aws:SourceIp': [\n                  '34.231.68.152/32',\n                  '34.197.76.213/32',\n                  '35.171.78.91/32',\n                  '35.169.138.21/32',\n                  '52.70.27.159/32',\n                  '52.44.63.161/32',\n                ],\n              },\n            },\n            effect: Effect.DENY,\n          }),\n        ],\n      }),\n</code></pre>"},{"location":"projects/classification-with-intercom/#add-teams","title":"Add Teams","text":"<p>Add several teams to your Intercom account. These are the potential teams that a ticket will be routed to. In this example, we are using the following teams to simulate an insurance company:</p> <pre><code>Account\nBilling\nClaims\nPolicy\nSales\nSupport\nTechnical\n</code></pre> <p></p>"},{"location":"projects/classification-with-intercom/#workspace-app","title":"Workspace App","text":"<p>In the General section of your workspace, copy the App ID that is being used.</p> <p></p>"},{"location":"projects/classification-with-intercom/#client-configuration","title":"Client Configuration","text":"<p>With this App ID, we will finally configure the client to use this App ID.</p> <p>Create and configure the <code>./intercom-client/.env</code> file with this App ID:</p> <pre><code>VITE_INTERCOM_APP_ID='xxxxxx'\n</code></pre>"},{"location":"projects/classification-with-intercom/#launch-client","title":"Launch Client","text":"<pre><code>cd intercom-client\nyarn dev\n</code></pre>"},{"location":"projects/classification-with-intercom/#integration-in-action","title":"Integration in Action","text":"<p>Once everything is deployed and configured, we can see the integration in action. After starting the client, navigate to <code>http://localhost:5173/</code></p> <p></p> <p>You should see the Intercom chat widget in the lower right corner.</p> <p></p> <p>Open that widget so we can create a Support Request.</p> <p></p> <p>Enter a Title and Description for the request.</p> <p>For example:</p> <pre><code>Title: Accident\nDescription: I was just in an accident and don't know what to do. Please help.\n</code></pre> <p>When this request is submitted, it will initially be <code>Unassigned</code>. However, we want to automatically assign this ticket to the correct team. After processing with Amazon Bedrock, this ticket will be assigned to the Claims team.</p> <p></p>"},{"location":"projects/classification-with-intercom/#how-it-works","title":"How it works","text":"<p>In order to assign this ticket, we will be passing the request information from Intercom to our Lambda function. This is done through Webhooks from Intercom.</p> <p>The notification will look something like this:</p> <pre><code>{\n  \"type\": \"notification_event\",\n  \"app_id\": \"xxxxxx\",\n  \"data\": {\n    \"type\": \"notification_event_data\",\n    \"item\": {\n      \"type\": \"ticket\",\n      \"id\": \"25\",\n      \"ticket_id\": \"21\",\n      \"ticket_attributes\": {\n        \"_default_title_\": \"Accident\",\n        \"_default_description_\": \"I was just in an accident and don't know what to do.  Please help.  \",\n        \"Platforms\": null,\n        \"Root cause\": null\n      }\n    }\n  }\n}\n</code></pre> <p>In order to route this to the correct team, we need to know what teams and admins are available. We will query Intercom for this information and use it when we're updating the ticket.</p> <pre><code>const response = await fetch('https://api.intercom.io/teams', {\n  method: 'GET',\n  headers: {\n    'Intercom-Version': '2.11',\n    'Authorization': `Bearer ${intercomToken}`,\n  },\n});\n\nconst response = await fetch('https://api.intercom.io/admins', {\n  method: 'GET',\n  headers: {\n    'Intercom-Version': '2.11',\n    'Authorization': `Bearer ${intercomToken}`,\n  },\n});\n</code></pre>"},{"location":"projects/classification-with-intercom/#classify","title":"Classify","text":"<p>To classify the ticket correctly, we will create a multi-shot example prompt to help Claude route it to the correct team using labeled categories.</p> <pre><code>const categories = `&lt;category&gt; \n    &lt;label&gt;Billing Inquiries&lt;/label&gt;\n    &lt;content&gt; Questions about invoices, charges, fees, and premiums Requests for clarification on billing statements Inquiries about payment methods and due dates \n    &lt;/content&gt; \n&lt;/category&gt; \n&lt;category&gt; \n    &lt;label&gt;Policy Administration&lt;/label&gt;\n    &lt;content&gt; Requests for policy changes, updates, or cancellations Questions about policy renewals and reinstatements Inquiries about adding or removing coverage options \n    &lt;/content&gt; \n&lt;/category&gt; \n&lt;category&gt; \n    &lt;label&gt;Claims Assistance&lt;/label&gt; \n    &lt;content&gt; Questions about the claims process and filing procedures Requests for help with submitting claim documentation Inquiries about claim status and payout timelines \n    &lt;/content&gt; \n&lt;/category&gt; \n&lt;category&gt; \n    &lt;label&gt;Coverage Explanations&lt;/label&gt; \n    &lt;content&gt; Questions about what is covered under specific policy types Requests for clarification on coverage limits and exclusions Inquiries about deductibles and out-of-pocket expenses \n    &lt;/content&gt; \n&lt;/category&gt; \n&lt;category&gt; \n    &lt;label&gt;Quotes and Proposals&lt;/label&gt; \n    &lt;content&gt; Requests for new policy quotes and price comparisons Questions about available discounts and bundling options Inquiries about switching from another insurer \n    &lt;/content&gt; \n&lt;/category&gt; \n&lt;category&gt; \n    &lt;label&gt;Account Management&lt;/label&gt; \n    &lt;content&gt; Requests for login credentials or password resets Questions about online account features and functionality Inquiries about updating contact or personal information \n    &lt;/content&gt; \n&lt;/category&gt; \n&lt;category&gt; \n    &lt;label&gt;Billing Disputes&lt;/label&gt; \n    &lt;content&gt; Complaints about unexpected or incorrect charges Requests for refunds or premium adjustments Inquiries about late fees or collection notices \n    &lt;/content&gt; \n&lt;/category&gt; \n&lt;category&gt; \n    &lt;label&gt;Claims Disputes&lt;/label&gt; \n    &lt;content&gt; Complaints about denied or underpaid claims Requests for reconsideration of claim decisions Inquiries about appealing a claim outcome \n    &lt;/content&gt; \n&lt;/category&gt; \n&lt;category&gt; \n    &lt;label&gt;Policy Comparisons&lt;/label&gt; \n    &lt;content&gt; Questions about the differences between policy options Requests for help deciding between coverage levels Inquiries about how policies compare to competitors' offerings \n    &lt;/content&gt; \n&lt;/category&gt; \n&lt;category&gt; \n    &lt;label&gt;General Inquiries&lt;/label&gt; \n    &lt;content&gt; Questions about company contact information or hours of operation Requests for general information about products or services Inquiries that don't fit neatly into other categories \n    &lt;/content&gt; \n&lt;/category&gt;`;\n\nexport async function classifyTicket(ticketContent: string): Promise&lt;string&gt; {\n  const prompt = `\n    You will classify a customer support ticket into one of the following categories:\n    &lt;categories&gt;\n        ${categories}\n    &lt;/categories&gt;\n\n    Here is the customer support ticket:\n    &lt;ticket&gt;\n        ${ticketContent}\n    &lt;/ticket&gt;\n\n    Respond with just the label of the category between category tags.\n  `;\n\n  return invokeBedrock(prompt);\n}\n\nexport async function invokeBedrock(prompt: string): Promise&lt;string&gt; {\n  const params = {\n    modelId: BEDROCK_MODEL_ID,\n    contentType: 'application/json',\n    accept: 'application/json',\n    body: JSON.stringify({\n      anthropic_version: 'bedrock-2023-05-31',\n      max_tokens: 4096,\n      messages: [\n        { role: 'user', content: prompt },\n        { role: 'assistant', content: '&lt;category&gt;' },\n      ],\n      temperature: 0,\n      top_p: 1,\n      stop_sequences: ['&lt;/category&gt;'],\n    }),\n  };\n\n  try {\n    const command = new InvokeModelCommand(params);\n    const response = await bedrockClient.send(command);\n\n    const responseBody = JSON.parse(new TextDecoder().decode(response.body));\n    return responseBody.content[0].text.trim();\n  } catch (error) {\n    console.error('Error invoking Bedrock:', error);\n    throw error;\n  }\n}\n</code></pre> <p>When this prompt passed to Bedrock, Claude Haiku will be used to classify the ticket using one of the available teams. This classification could be further enhanced with RAG to provide even more context to Claude. In this simple example, we will use just these examples.</p> <p>Once classified, an API request is made to Intercom to update the ticket with the new assignee.</p> <pre><code>const response = await fetch(`https://api.intercom.io/tickets/${ticketId}`, {\n  method: 'PUT',\n  headers: {\n    'Content-Type': 'application/json',\n    'Accept': 'application/json',\n    'Intercom-Version': '2.11',\n    'Authorization': `Bearer ${intercomToken}`,\n  },\n  body: JSON.stringify({\n    assignment: {\n      admin_id: selectedAdmin.id,\n      assignee_id: assigneeId,\n    },\n    state: 'in_progress',\n  }),\n});\n</code></pre>"},{"location":"projects/classification-with-intercom/#clean-up","title":"Clean up","text":"<p>To remove the AWS resources deployed as part of this demo:</p> <pre><code>yarn cdk destroy\n</code></pre>"},{"location":"projects/claude-tools-chatbot/","title":"Claude Tools Chatbot","text":""},{"location":"projects/claude-tools-chatbot/#creating-a-chatbot-with-claude-tool-use-and-amazon-bedrock-converse-api","title":"Creating a ChatBot with Claude Tool use and Amazon Bedrock Converse API","text":"<p>This example will demonstrate how to build a simple ChatBot application using Claude Tools and the Amazon Bedrock Converse API.</p> <p></p> <p>This demo will deploy a simple client that will interact with an AWS AppSync GraphQL API. This API will use an AWS Lambda resolver to make requests to Amazon Bedrock with the Amazon Bedrock Converse API. These requests and responses are stored in an Amazon DynamoDB table via the AppSync API that allows for easy updates in the client. With these tools, we can create a chat bot that is able to look up orders in a database by using the tool use feature of Amazon Bedrock.</p> <ul> <li>Creating a ChatBot with Claude Tool use and Amazon Bedrock Converse API</li> <li>Infrastructure<ul> <li>AppSync</li> <li>VPC</li> <li>RDS</li> <li>Lambda Resolver</li> </ul> </li> <li>Chat Bot Interactions<ul> <li>Bedrock Converse Client Configuration</li> <li>Basic configuration</li> <li>Tool Config</li> <li>System Prompt</li> <li>Messages</li> <li>Basic Flow</li> <li>Tools Required</li> <li>Using the tool</li> <li>Continued conversation</li> </ul> </li> <li>Using the Demo<ul> <li>Deploying</li> <li>Logging In</li> <li>Example Conversations</li> <li>Order Lookup</li> <li>Cancel Order</li> <li>Account Lookup</li> <li>Combining Requests</li> <li>Cleanup</li> </ul> </li> </ul>"},{"location":"projects/claude-tools-chatbot/#infrastructure","title":"Infrastructure","text":"<p>The infrastructure in this demo is deployed using AWS Cloud Development Kit (CDK) and creates all of the necessary components for you.</p>"},{"location":"projects/claude-tools-chatbot/#appsync","title":"AppSync","text":"<pre><code>this.graphqlApi = new GraphqlApi(this, 'graphqlApi', {\n  name: 'ClaudeTools',\n  definition: {\n    schema: SchemaFile.fromAsset('./src/resources/graphql/schema.graphql'),\n  },\n  logConfig: {\n    retention: RetentionDays.ONE_WEEK,\n    fieldLogLevel: FieldLogLevel.ALL,\n  },\n  authorizationConfig: {\n    defaultAuthorization: {\n      authorizationType: AuthorizationType.USER_POOL,\n      userPoolConfig: {\n        userPool: props.userPool,\n      },\n    },\n    additionalAuthorizationModes: [\n      {\n        authorizationType: AuthorizationType.API_KEY,\n        apiKeyConfig: {\n          expires: Expiration.after(Duration.days(365)),\n        },\n      },\n    ],\n  },\n  xrayEnabled: true,\n});\n</code></pre> <p>To create the AppSync API, we will use the defined schema. The schema is the foundation of the GraphQL API and serves as a blueprint for the data will be using.</p> <p>Next we will define two data sources for our API:</p> <pre><code>const messageProcessorDataSource = this.graphqlApi.addLambdaDataSource(\n  'MessageProcessorDataSource',\n  props.messageProcessorLambda,\n);\n\nconst conversationDataSource = this.graphqlApi.addDynamoDbDataSource(\n  'ConversationDataSource',\n  props.conversationTable,\n);\n</code></pre> <p>The <code>messageProcessorLambda</code> will be where all of the interactions with Bedrock occur. When a user interacts with the client, the messages they type will be sent to this Lambda function. The <code>conversationTable</code> is where the chat messages are stored. This is also used to pass the information back to the client through an AppSync subscription. When a mutation occurs, the subscription will be triggered and the client updated with the new data.</p> <p>We control this by adding resolvers to the data sources:</p> <pre><code>messageProcessorDataSource.createResolver('ProcessMessage', {\n  typeName: 'Mutation',\n  fieldName: 'processMessage',\n  requestMappingTemplate: MappingTemplate.fromFile(\n    './src/resources/graphql/Mutation.ProcessMessage.req.vtl',\n  ),\n  responseMappingTemplate: MappingTemplate.fromFile(\n    './src/resources/graphql/Mutation.ProcessMessage.res.vtl',\n  ),\n});\n\nconversationDataSource.createResolver('UpdateConversation', {\n  typeName: 'Mutation',\n  fieldName: 'updateConversation',\n  requestMappingTemplate: MappingTemplate.fromFile(\n    './src/resources/graphql/Mutation.UpdateConversation.req.vtl',\n  ),\n  responseMappingTemplate: MappingTemplate.fromFile(\n    './src/resources/graphql/Mutation.UpdateConversation.res.vtl',\n  ),\n});\n</code></pre> <p>Now, when the <code>processMessage</code> API is invoked it will use the <code>messageProcessorDataSource</code> and when the <code>updateConversation</code> API is invoked it will use the <code>conversationDataSource</code>.</p>"},{"location":"projects/claude-tools-chatbot/#vpc","title":"VPC","text":"<p>This demo uses an AWS PrivateLink to access Bedrock. Later we will deploy other services to this Amazon Virtual Private Cloud (Amazon VPC).</p> <pre><code>this.vpc = new Vpc(this, 'VPC', {\n  natGateways: 1,\n  subnetConfiguration: [\n    {\n      cidrMask: 24,\n      name: 'PrivateWithEgress',\n      subnetType: SubnetType.PRIVATE_WITH_EGRESS,\n    },\n    {\n      cidrMask: 24,\n      name: 'Public',\n      subnetType: SubnetType.PUBLIC,\n    },\n  ],\n});\n\nthis.securityGroup = new SecurityGroup(this, 'SecurityGroup', {\n  vpc: this.vpc,\n  description: 'Security Group',\n  allowAllOutbound: true,\n});\n\nthis.bedrockInterfaceEndpoint = this.vpc.addInterfaceEndpoint(\n  'BedrockAccessPoint',\n  {\n    service: InterfaceVpcEndpointAwsService.BEDROCK_RUNTIME,\n    privateDnsEnabled: true,\n  },\n);\n\nthis.bedrockInterfaceEndpoint.addToPolicy(\n  new PolicyStatement({\n    principals: [new AnyPrincipal()],\n    actions: ['bedrock:InvokeModel'],\n    resources: ['arn:aws:bedrock:*::foundation-model/*'],\n  }),\n);\n</code></pre> <p>This VPC creates two Subnets - a Public and Private with Egress subnet. This will also create a NAT Gateway in this VPC. Most importantly though, it create an interface VPC endpoint that will allow access to Bedrock within the VPC. Now, when a resource within the VPC makes a request to Bedrock, it will do so over a private connection.</p>"},{"location":"projects/claude-tools-chatbot/#rds","title":"RDS","text":"<p>This demo includes a database that we will use as one of our tools. This database will store customer and order information that will be retrieved by our Lambda resolver function. We will put this database in the same private subnet in our VPC with our Lambda function so all of the communication occurs over our private connection.</p> <pre><code>this.database = new DatabaseInstance(this, 'database', {\n  engine: DatabaseInstanceEngine.POSTGRES,\n  vpc: props.vpc,\n  vpcSubnets: { subnetType: SubnetType.PRIVATE_WITH_EGRESS },\n  instanceType: InstanceType.of(\n    InstanceClass.BURSTABLE4_GRAVITON,\n    InstanceSize.MEDIUM,\n  ),\n  multiAz: false,\n  allowMajorVersionUpgrade: true,\n  autoMinorVersionUpgrade: true,\n  backupRetention: Duration.days(0),\n  securityGroups: [props.securityGroup],\n});\n\nthis.database.connections.allowInternally;\n</code></pre> <p>This database will be built with Amazon Relational Database Service (RDS) and use PostgreSQL as the database system. During deployment of the CDK, we will also initialize the database with our fake customer and order data using a Custom Resource.</p> <pre><code>def on_create(event):\n    try:\n        create_tables()\n        load_data()\n        physical_id = \"CreateTablesAndLoadData\"\n        return {\"PhysicalResourceId\": physical_id}\n    except Exception as e:\n        print(f\"Error in on_create: {e}\")\n        raise e\n</code></pre> <p>With the database created and initialized, we will have access to it from our other resources.</p>"},{"location":"projects/claude-tools-chatbot/#lambda-resolver","title":"Lambda Resolver","text":"<p>As noted above, our Lambda resolver will be used to interact with Bedrock and our database. All of these resources are created in a private subnet of our VPC and will need access to those resources through the private connection.</p> <p>To ensure that the Lambda uses this private connection when making requests to Bedrock, we will assign it an IAM Role with a conditional IAM Policy that allows <code>InvokeModel</code> only through the specific VPC interface endpoint.</p> <pre><code>const resolverLambdaRole = new Role(this, 'resolverLambdaRole', {\n  assumedBy: new ServicePrincipal('lambda.amazonaws.com'),\n  inlinePolicies: {\n    ['bedrock']: new PolicyDocument({\n      statements: [\n        new PolicyStatement({\n          resources: ['*'],\n          actions: ['bedrock:InvokeModel'],\n          conditions: {\n            'ForAnyValue:StringEquals': {\n              'aws:sourceVpce': [props.bedrockInterfaceEndpoint.vpcEndpointId],\n            },\n          },\n        }),\n      ],\n    }),\n  },\n});\n</code></pre> <p>Because the boto3 version in our Lambda uses the <code>Converse</code> API feature and the current default Lambda runtime might not include this, we need to build our Lambda function using Docker before deploying it with our CDK. This requires running Docker daemon on the device deploying the CDK, but ensures that the Lambda has all of the necessary and up to date packages for using the latest features.</p> <pre><code>this.resolverLambda = new Function(this, 'resolverLambda', {\n  code: Code.fromAsset(path.join(__dirname, 'resources/resolverLambda'), {\n    bundling: {\n      image: Runtime.PYTHON_3_12.bundlingImage,\n      command: [\n        'bash',\n        '-c',\n        'pip install -r requirements.txt -t /asset-output &amp;&amp; cp -au . /asset-output',\n      ],\n    },\n  }),\n  runtime: Runtime.PYTHON_3_12,\n  vpc: props.vpc,\n  vpcSubnets: { subnetType: SubnetType.PRIVATE_WITH_EGRESS },\n  architecture: Architecture.ARM_64,\n  handler: 'index.handler',\n  timeout: Duration.minutes(5),\n  role: resolverLambdaRole,\n  environment: {\n    RDS_SECRET_NAME: props.database.secret?.secretName!,\n  },\n});\n</code></pre> <p>Now, when we deploy our CDK, we will install the dependencies in our <code>requirements.txt</code> file before bundling and zipping the Lambda function.</p>"},{"location":"projects/claude-tools-chatbot/#chat-bot-interactions","title":"Chat Bot Interactions","text":""},{"location":"projects/claude-tools-chatbot/#bedrock-converse-client-configuration","title":"Bedrock Converse Client Configuration","text":"<p>With our infrastructure deployed, we're ready to interact with our chat bot. To do this, we will make a request from our client. Every request we make will follow the same basic pattern and use the same request.</p> <pre><code>    response = bedrock_client.converse(\n        modelId=model_id,\n        messages=converted_messages,\n        system=system_prompts,\n        toolConfig=tool_config,\n        inferenceConfig=inference_config,\n        additionalModelRequestFields=additional_model_fields,\n    )\n</code></pre> <p>Each time we make this request, we will provide Claude the information it needs to process the request.</p>"},{"location":"projects/claude-tools-chatbot/#basic-configuration","title":"Basic configuration","text":"<pre><code>temperature = 0\ntop_k = 10\nmax_tokens = 4096\ninference_config = {\"temperature\": temperature, \"maxTokens\": max_tokens}\nadditional_model_fields = {\"top_k\": top_k}\n</code></pre> <p>Each of these are statically defined for every request.</p>"},{"location":"projects/claude-tools-chatbot/#tool-config","title":"Tool Config","text":"<p>Next we will tell Claude what tools it has access to with <code>toolConfig</code>:</p> <pre><code>tool_config = {\n    \"toolChoice\": {\"auto\": {}},\n    \"tools\": [\n        {\n            \"toolSpec\": {\n                \"name\": \"get_user\",\n                \"description\": \"Looks up a user by email, phone, or username.\",\n                ...\n            },\n        },\n        {\n            \"toolSpec\": {\n                \"name\": \"get_order_by_id\",\n                \"description\": \"Retrieves the details of a specific order based on the order ID. Returns the order ID, product name, quantity, price, and order status.\",\n                ...\n            },\n        },\n        {\n            \"toolSpec\": {\n                \"name\": \"get_customer_orders\",\n                \"description\": \"Retrieves the list of orders belonging to a user based on a user's customer id.\",\n                ...\n            },\n        },\n        {\n            \"toolSpec\": {\n                \"name\": \"cancel_order\",\n                \"description\": \"Cancels an order based on a provided order_id.  Only orders that are 'processing' can be cancelled\",\n                ...\n            },\n        },\n    ],\n}\n</code></pre> <p>Each of these tools has a defined schema and description that tells Claude what it can do and what is required to use. For example:</p> <pre><code>{\n    \"toolSpec\": {\n        \"name\": \"get_order_by_id\",\n        \"description\": \"Retrieves the details of a specific order based on the order ID. Returns the order ID, product name, quantity, price, and order status.\",\n        \"inputSchema\": {\n            \"json\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"order_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unique identifier for the order.\",\n                    }\n                },\n                \"required\": [\"order_id\"],\n            },\n        },\n    },\n}\n</code></pre> <p>This tells Claude that it has access to a function that can look up an order if it knows what the <code>order_id</code> is. This also defines the <code>inputSchema</code> so that Claude knows what the properties the tool uses and what is required. We'll see how to use this later.</p>"},{"location":"projects/claude-tools-chatbot/#system-prompt","title":"System Prompt","text":"<p>Next, wee will define a system prompt that lets Claude know that tools are available to use if needed, but aren't necessary.</p> <pre><code>system_prompt = \"\"\"\nYou are a customer support chat bot for an online retailer called AnyCompany.\nYour job is to help users look up their account, orders, and cancel orders.\nBe helpful and brief in your responses.\nYou have access to a set of tools, but only use them when needed.\nIf you do not have enough information to use a tool correctly, ask a user follow up questions to get the required inputs.\nDo not call any of the tools unless you have the required data from a user.\n\nIn each conversational turn, you will begin by thinking about your response.\nUse &lt;thinking&gt;&lt;/thinking&gt; to think through the process step by step ensuring that you have all of the required input.\nOnce you're done, you will write a user-facing response.\nIt's important to place all user-facing conversational responses in &lt;reply&gt;&lt;/reply&gt; XML tags to make them easy to parse.\n\"\"\"\n</code></pre>"},{"location":"projects/claude-tools-chatbot/#messages","title":"Messages","text":"<p>Finally, we will define the <code>messages</code> as the entire turn by turn history of the conversation. This will change each time we make a request by appending the latest request with all of the previous requests so that Claude has the entire context of the conversation.</p>"},{"location":"projects/claude-tools-chatbot/#basic-flow","title":"Basic Flow","text":"<p>In a basic flow where tool use is not required:</p> <ul> <li>The user sends a request to the Lambda resolver through the AppSync API</li> <li>The Lambda resolver queries the conversation Table for the <code>conversationId</code></li> <li>The Lambda resolver appends to (for an existing conversation) or creates (for a new conversation) an item in the conversation Table</li> <li>This mutation triggers the AppSync subscription to update the client</li> <li>The Lambda resolver makes a request to Bedrock using the Converse API with the entire conversation</li> <li>Bedrock responds</li> <li>The Lambda resolver appends the existing item in the conversation Table with the response</li> <li>This mutation triggers the AppSync subscription to update the client</li> </ul> <p></p> <p>For example:</p> <pre><code>User: I need help with an order\n</code></pre> <p>This message is passed to Bedrock along with the system prompt. The response from Bedrock will look something like this:</p> <pre><code>{\n  \"output\": {\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"text\": \"Okay, let's see how I can assist you with your order. &lt;thinking&gt;\\nTo look up details about an order, I will need the order ID. I should ask the user for that first.\\n&lt;/thinking&gt;\\n\\n&lt;reply&gt;\\nSure, I'd be happy to help you with your order. Can you please provide me with the order ID? This will allow me to look up the specific details of your order.\\n&lt;/reply&gt;\"\n        }\n      ]\n    }\n  },\n  \"stopReason\": \"end_turn\"\n}\n</code></pre> <p>We can see that Claude used <code>&lt;thinking&gt;</code> to determine that it needs additional information to help the user. However, the <code>stopReason</code> is still <code>end_turn</code> because Claude has been instructed to only use tools when all of the information required is available. In this case, it still needs an order ID.</p> <pre><code>Bot:  Sure, I'd be happy to help you with your order. Can you please provide me with the order ID? This will allow me to look up the specific details of your order.\n</code></pre> <p></p>"},{"location":"projects/claude-tools-chatbot/#tools-required","title":"Tools Required","text":"<p>At this point in the flow, Claude has prompted the user to provide an Order ID because it needs that information in order to retrieve the order details.</p> <pre><code>User: my order is 14826\n</code></pre> <p>As we continue to append the conversation with each turn, we now send this to Bedrock:</p> <pre><code>[\n  {\n    \"content\": [\n      {\n        \"text\": \"i need help with an order\"\n      }\n    ],\n    \"role\": \"user\"\n  },\n  {\n    \"content\": [\n      {\n        \"text\": \"Okay, let's see how I can assist you with your order. &lt;thinking&gt;\\nTo look up details about an order, I will need the order ID. I should ask the user for that first.\\n&lt;/thinking&gt;\\n\\n&lt;reply&gt;\\nSure, I'd be happy to help you with your order. Can you please provide me with the order ID? This will allow me to look up the specific details of your order.\\n&lt;/reply&gt;\"\n      }\n    ],\n    \"role\": \"assistant\"\n  },\n  {\n    \"content\": [\n      {\n        \"text\": \"my order is 14826\"\n      }\n    ],\n    \"role\": \"user\"\n  }\n]\n</code></pre> <p>This <code>messages</code> contains the entire history of the conversation so that Claude has all of the information needed. Because Claude has no \"memory\" of previous interactions, each time we make a new request, we need to tell it everything that has happened already.</p> <p>Because Claude knows that it has tools available to it (as defined in the <code>toolConfig</code>) and it has the required information (the order ID), the response from Bedrock will have a <code>stopReason</code> of <code>tool_use</code> and the information needed to use the tool.</p> <pre><code>{\n  \"output\": {\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"text\": \"Okay, got it. &lt;thinking&gt;\\nThe user provided the order ID 14826. I can use the \\\"get_order_by_id\\\" tool to retrieve the details for this order:\"\n        },\n        {\n          \"toolUse\": {\n            \"toolUseId\": \"tooluse_D37XtLaVTOCceSv_GMn1wA\",\n            \"name\": \"get_order_by_id\",\n            \"input\": {\n              \"order_id\": \"14826\"\n            }\n          }\n        }\n      ]\n    }\n  },\n  \"stopReason\": \"tool_use\"\n}\n</code></pre>"},{"location":"projects/claude-tools-chatbot/#using-the-tool","title":"Using the tool","text":"<p>Now our code knows that Claude wants to use a tool. But Claude cannot use the tool itself. This is where we need to step in and process the request for Claude. This changes the flow to include a direct request to our database.</p> <p></p> <p>We continue to update the conversation Table with each turn, but because these steps do not need to be exposed to the user, we do not need to include updates to the subscription.</p> <p>We will make an SQL request to our database using the <code>order_id</code> provided to us by Claude. Because Claude knows the input schema of our tool, it is able to provide a response in the desired shape. This was defined here as part of the <code>toolConfig</code> passed to Claude:</p> <pre><code>        \"inputSchema\": {\n            \"json\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"order_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unique identifier for the order.\",\n                    }\n                },\n                \"required\": [\"order_id\"],\n            },\n        },\n</code></pre> <p>That allows us to extract the information from the response:</p> <pre><code>{\n  \"toolUse\": {\n    \"toolUseId\": \"tooluse_D37XtLaVTOCceSv_GMn1wA\",\n    \"name\": \"get_order_by_id\",\n    \"input\": {\n      \"order_id\": \"14826\"\n    }\n  }\n}\n</code></pre> <p>Now, when we call our <code>process_tool_call</code> function, we are able to determine what the SQL should look like based on the <code>tool_name</code> that Claude wants to use and return the results.</p> <pre><code>            elif tool_name == \"get_order_by_id\":\n                order_id = tool_input[\"order_id\"]\n                query = \"SELECT * FROM orders WHERE id = %s\"\n                logging.info(\n                    f\"Executing query: {query} with order_id: {order_id}\"\n                )\n                cursor.execute(query, (order_id,))\n                order = cursor.fetchone()\n                if order:\n                    order_dict = dict(\n                        zip(\n                            [\n                                \"id\",\n                                \"customer_id\",\n                                \"product\",\n                                \"quantity\",\n                                \"price\",\n                                \"status\",\n                            ],\n                            order,\n                        )\n                    )\n                    logging.info(f\"Order found: {order_dict}\")  # Log the found order\n                    return order_dict\n                else:\n                    logging.info(f\"Order not found with ID: {order_id}\")\n                    return None\n</code></pre> <pre><code>SELECT * FROM orders WHERE id = %s with order_id: 14826\n</code></pre> <p>After we get the response from our database, we will append that message as a <code>user</code> turn and send it back to Bedrock.</p> <pre><code>[\n  {\n    \"content\": [\n      {\n        \"text\": \"i need help with an order\"\n      }\n    ],\n    \"role\": \"user\"\n  },\n  {\n    \"content\": [\n      {\n        \"text\": \"Okay, let's see how I can assist you with your order. &lt;thinking&gt;\\nTo look up details about an order, I will need the order ID. I should ask the user for that first.\\n&lt;/thinking&gt;\\n\\n&lt;reply&gt;\\nSure, I'd be happy to help you with your order. Can you please provide me with the order ID? This will allow me to look up the specific details of your order.\\n&lt;/reply&gt;\"\n      }\n    ],\n    \"role\": \"assistant\"\n  },\n  {\n    \"content\": [\n      {\n        \"text\": \"my order is 14826\"\n      }\n    ],\n    \"role\": \"user\"\n  },\n  {\n    \"role\": \"assistant\",\n    \"content\": [\n      {\n        \"text\": \"Okay, got it. &lt;thinking&gt;\\nThe user provided the order ID 14826. I can use the \\\"get_order_by_id\\\" tool to retrieve the details for this order:\"\n      },\n      {\n        \"toolUse\": {\n          \"toolUseId\": \"tooluse_D37XtLaVTOCceSv_GMn1wA\",\n          \"name\": \"get_order_by_id\",\n          \"input\": \"{\\\"order_id\\\": \\\"14826\\\"}\"\n        }\n      }\n    ]\n  },\n  {\n    \"role\": \"user\",\n    \"content\": [\n      {\n        \"toolResult\": {\n          \"toolUseId\": \"tooluse_D37XtLaVTOCceSv_GMn1wA\",\n          \"content\": [\n            {\n              \"text\": \"{\\\"id\\\": \\\"14826\\\", \\\"customer_id\\\": \\\"678902\\\", \\\"product\\\": \\\"Backpack\\\", \\\"quantity\\\": 1, \\\"price\\\": 59.99, \\\"status\\\": \\\"Shipped\\\"}\"\n            }\n          ],\n          \"status\": \"success\"\n        }\n      }\n    ]\n  }\n]\n</code></pre> <p>Now Claude has the result of the tool that it wanted to use, we will get back a new response.</p> <pre><code>{\n  \"output\": {\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"text\": \"The tool returned the order details - it's for 1 Backpack that has already shipped, with a total price of $59.99. I can provide this information to the user.\\n&lt;/thinking&gt;\\n\\n&lt;reply&gt;\\nI was able to look up your order #14826. It was for 1 Backpack at $59.99. The order status shows that it has been shipped already. Please let me know if you need any other details about this order.\\n&lt;/reply&gt;\"\n        }\n      ]\n    }\n  },\n  \"stopReason\": \"end_turn\"\n}\n</code></pre> <p>After we've used the tool, and Claude doesn't need to use it again, we get a <code>stopReason</code> of <code>end_turn</code> and can return this response back to the user.</p> <p>The process to do this in the Lambda resolver looks something like this:</p> <pre><code>    bot_response, stop_reason = process_message(messages)\n    updated_conversation = update_conversation(conversation_id, owner_id, bot_response)\n    while stop_reason == \"tool_use\":\n        messages.append(bot_response)\n        tool_response = use_tool(messages)\n        messages.append(tool_response)\n        update_conversation(conversation_id, owner_id, tool_response)\n        bot_response, stop_reason = process_message(messages)\n        update_conversation(conversation_id, owner_id, bot_response)\n</code></pre> <p>While Claude wants to use a tool, we will continue to loop through this process.</p> <p></p>"},{"location":"projects/claude-tools-chatbot/#continued-conversation","title":"Continued conversation","text":"<p>We can continue this conversation with additional requests.</p> <pre><code>User: I'd like to cancel this order\n</code></pre> <pre><code>[\n  {\n    \"content\": [\n      {\n        \"text\": \"I'd like to cancel this order\"\n      }\n    ],\n    \"role\": \"user\"\n  },\n  {\n    \"role\": \"assistant\",\n    \"content\": [\n      {\n        \"text\": \"Okay, got it. &lt;thinking&gt;\\nThe user wants to cancel order #14826 for the Backpack. To cancel an order, I need to use the \\\"cancel_order\\\" tool and provide the order_id.\\n\\nFirst, let me double check the status of the order:\"\n      },\n      {\n        \"toolUse\": {\n          \"toolUseId\": \"tooluse_TGy4SjAgTve9nH4u5ePRbg\",\n          \"name\": \"get_order_by_id\",\n          \"input\": \"{\\\"order_id\\\": \\\"14826\\\"}\"\n        }\n      }\n    ]\n  }\n]\n</code></pre> <p>Claude has recognized that the user wants to cancel the order and knows that it needs the order ID in order to do this. But because we are including the entire conversation history with each request, it already has the order ID and doesn't need to get that information from the user again. But it does want to double check the order status as it may have changed.</p> <p>When we pass this back to Bedrock, we get a response similar to this:</p> <pre><code>{\n  \"output\": {\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"text\": \"The status shows \\\"Shipped\\\", which means the order has already been fulfilled and shipped out.\\n\\nThe \\\"cancel_order\\\" tool states that only orders with a \\\"processing\\\" status can be cancelled. Since this order has already shipped, I cannot cancel it using the tool.\\n&lt;/thinking&gt;\\n\\n&lt;reply&gt;\\nUnfortunately, I am unable to cancel your order #14826 for the Backpack at this time. The order status shows that it has already been shipped. Orders can only be cancelled if they are still in the \\\"processing\\\" stage before being shipped out.\\n\\nSince your order has already shipped, it cannot be cancelled through our system. Please let me know if you need any other assistance regarding this order.\\n&lt;/reply&gt;\"\n        }\n      ]\n    }\n  },\n  \"stopReason\": \"end_turn\"\n}\n</code></pre> <p>Here we can see that even though Claude wants to use the <code>cancel_order</code> tool, that tool definition includes instructions about when it can be used:</p> <pre><code>\"toolSpec\": {\n    \"name\": \"cancel_order\",\n    \"description\": \"Cancels an order based on a provided order_id.  Only orders that are 'processing' can be cancelled\",\n    \"inputSchema\": {\n        \"json\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"order_id\": {\n                    \"type\": \"string\",\n                    \"description\": \"The order_id pertaining to a particular order\",\n                }\n            },\n            \"required\": [\"order_id\"],\n        },\n    },\n},\n</code></pre> <p>Because we have instructed Claude to only allow orders that are in a <code>processing</code> state to be canceled, the <code>stopReason</code> from Bedrock is <code>end_turn</code> instead of <code>tool_use</code> and the response is sent to the user.</p> <p></p>"},{"location":"projects/claude-tools-chatbot/#using-the-demo","title":"Using the Demo","text":""},{"location":"projects/claude-tools-chatbot/#deploying","title":"Deploying","text":"<p>To deploy the demo you'll need to meet the following prerequisites:</p> <ul> <li>AWS account</li> <li>AWS CLI configured</li> <li>yarn installed</li> <li>Docker installed</li> </ul> <p>To deploy the CDK:</p> <pre><code>git clone https://github.com/aws-samples/anthropic-on-aws.git\ncd anthropic-on-aws/claude-tools-chatbot\nyarn\nyarn launch\n</code></pre>"},{"location":"projects/claude-tools-chatbot/#logging-in","title":"Logging In","text":"<p>Once deployed, your demo will include output with an Amazon CloudFront distribution URL. You can use this to log in to the client by creating an Amazon Cognito user with a valid email address.</p> <p></p> <p>Once you have received the validation code from Cognito, you can log in and try out some questions. The data loaded in to the database can be found here</p>"},{"location":"projects/claude-tools-chatbot/#example-conversations","title":"Example Conversations","text":""},{"location":"projects/claude-tools-chatbot/#order-lookup","title":"Order Lookup","text":"<pre><code>I need help with an order\n</code></pre> <pre><code>My order is 32057\n</code></pre>"},{"location":"projects/claude-tools-chatbot/#cancel-order","title":"Cancel Order","text":"<pre><code>I need help with an order\n</code></pre> <pre><code>My order is 37129\n</code></pre> <pre><code>Please cancel my order\n</code></pre>"},{"location":"projects/claude-tools-chatbot/#account-lookup","title":"Account Lookup","text":"<pre><code>Hello, I need some help\n</code></pre> <pre><code>I don't remember my order id\n</code></pre> <pre><code>My email is anacarolina_silva@example.com\n</code></pre> <pre><code>What are my orders\n</code></pre>"},{"location":"projects/claude-tools-chatbot/#combining-requests","title":"Combining Requests","text":"<pre><code>I need to cancel an order but don't remember my order id\n</code></pre> <pre><code>My phone number is 312-555-8204\n</code></pre> <pre><code>Yes\n</code></pre>"},{"location":"projects/claude-tools-chatbot/#cleanup","title":"Cleanup","text":"<p>To remove the resources created:</p> <pre><code>yarn cdk destroy\n</code></pre>"},{"location":"projects/metaprompt-generator/","title":"Metaprompt Generator","text":""},{"location":"projects/metaprompt-generator/#anthropic-metaprompt-generator","title":"Anthropic MetaPrompt Generator","text":"<p>In this example, we will cover two concepts for using LLMs in general and Anthropic in particular. The former is a method of asynchronously processing a complex prompt with AWS serverless components. The later is a method of using Claude 3 Opus to generate a well constructed prompt for a specific task. This can be a good jumping off point for prompt creation. You'll be able to enter a simple task and have a well constructed prompt generated for you.</p>"},{"location":"projects/metaprompt-generator/#asynchronously-processing","title":"Asynchronously Processing","text":"<p>When working with Large Language Models and complex prompts, you will often exceed timeouts of various systems. In this case, if we simply had used an Amazon API Gateway with an AWS Lambda Function, the API Gateway timeout limit of 30 seconds could have been reached. In order to process the request with the LLM, we can use an AWS Step Function. By using a Step Function, we can immediately return a response to the API request and begin processing asynchronously.</p> <p></p> <p>This demo uses a simple React application hosted on an S3 Bucket with a Cloudfront Distribution in front of it. Within this application, we have created an API Gateway that is integrated with a Lambda function. This function will be used to start the state machine that will process the request. Within the state machine, the <code>promptGenerator</code> Lambda is invoked that will make a request to Anthropic with the included prompt and the task and variables from the client. When a response is returned from Anthropic, the Lambda function will update the AppSync database.</p> <ol> <li>Client makes POST request to API Gateway with task and optional variables</li> <li>The API Gateway invokes the <code>requestHandler</code> Lambda</li> <li>The <code>requestHandler</code> Lambda starts the state machine</li> <li>Within the state machine, the <code>promptGenerator</code> Lambda is invoked</li> <li>The <code>promptGenerator</code> Lambda makes a request to Anthropic</li> <li>Anthropic responds to the request</li> <li>The <code>promptGenerator</code> Lambda updates the AppSync database with the generated prompt</li> <li>When the AppSync database is updated, the client is updated through a GraphQL subscription</li> </ol>"},{"location":"projects/metaprompt-generator/#task-entry","title":"Task Entry","text":"<p>To start the process, enter a task in the Task Entry form and press Submit. This will send the request to the <code>requestHandler</code> Lambda function through the API Gateway.</p> <p></p>"},{"location":"projects/metaprompt-generator/#request-handler-lambda","title":"Request Handler Lambda","text":"<p>Because the <code>requestHandler</code> Lambda function is just acting as the front end for this application, it is relatively simple.</p> <pre><code>const params = {\n  stateMachineArn,\n  input: JSON.stringify({ task, variables }),\n};\n\nconst command = new StartExecutionCommand(params);\nawait sfnClient.send(command);\n\nreturn createApiResponse({\n  statusCode: 202,\n  body: JSON.stringify({ message: 'Prompt generation started' }),\n});\n</code></pre> <p>The core functionality is to start the state machine and return a response to the API Gateway and React application.</p>"},{"location":"projects/metaprompt-generator/#state-machine","title":"State Machine","text":"<p>In this example, the State Machine is kept very simple.</p> <p></p> <p>It's only job is to invoke the <code>promptGenerator</code> Lambda function.</p>"},{"location":"projects/metaprompt-generator/#prompt-generator-lambda","title":"Prompt Generator Lambda","text":""},{"location":"projects/metaprompt-generator/#pre-processing","title":"Pre-Processing","text":"<p>Before we make the request to Anthropic, we will perform pre-processing of the prompt. This will allow us to replace the <code>{{TASK}}</code> with the requested task and add the optional variables to the prompt.</p> <pre><code>const { task, variables = [] } = event;\n\nconst updatedPrompt = promptTemplate.replace('{{TASK}}', task);\nconsole.log(`Updated Prompt: ${updatedPrompt}`);\n\nlet variableString = '';\nvariableString = variables\n  .map((variable: string) =&gt; `{${variable.toUpperCase()}}`)\n  .join('\\n');\n\nlet assistantPartial = '';\nif (variableString) {\n  assistantPartial += '&lt;Inputs&gt;';\n  assistantPartial += variableString + '\\n&lt;/Inputs&gt;\\n';\n}\nassistantPartial += '&lt;Instructions Structure&gt;';\nconsole.log(`AssistantPartial: \\n${assistantPartial}`);\n</code></pre> <p>If no variables are in the request, no <code>&lt;Inputs&gt;&lt;/Inputs&gt;</code> will be added.</p> <p>We will also make a mutation to our AppSync database to let the user know that we're generating their prompt.</p> <pre><code>const generatingResponse = await apolloClient.mutate({\n  mutation: UPDATE_PROMPT_MUTATION,\n  variables: {\n    id: promptId,\n    status: 'GENERATING',\n  },\n});\n</code></pre> <p></p>"},{"location":"projects/metaprompt-generator/#anthropic-request","title":"Anthropic Request","text":"<p>Now that our prompt has been finalized, we will make the request using the Anthropic SDK using the <code>updatedPrompt</code> and <code>assistantPartial</code>.</p> <pre><code>const response = await anthropic.messages.create({\n  max_tokens: 4096,\n  temperature: 0,\n  messages: [\n    { role: 'user', content: updatedPrompt },\n    { role: 'assistant', content: assistantPartial },\n  ],\n  model: 'claude-3-opus-20240229',\n});\n</code></pre>"},{"location":"projects/metaprompt-generator/#post-processing","title":"Post Processing","text":"<p>To make the result easier to read, we will extract the results between the tags from the response and remove any empty tags. Once completed, we will have a simple to read and use prompt.</p> <pre><code>function extractBetweenTags(\n  tag: string,\n  text: string,\n  strip: boolean = false,\n): string[] {\n  const regex = new RegExp(`&lt;${tag}&gt;(.+?)&lt;/${tag}&gt;`, 'gs');\n  const matches = text.match(regex);\n  if (matches) {\n    return strip\n      ? matches.map((match) =&gt; match.replace(regex, '$1').trim())\n      : matches.map((match) =&gt; match.replace(regex, '$1'));\n  }\n  return [];\n}\n\nfunction removeEmptyTags(text: string): string {\n  return text.replace(/&lt;(\\w+)&gt;&lt;\\/\\1&gt;$/g, '');\n}\n\nfunction extractPrompt(metapromptResponse: string): string {\n  const betweenTags = extractBetweenTags('Instructions', metapromptResponse)[0];\n  return removeEmptyTags(removeEmptyTags(betweenTags).trim()).trim();\n}\n</code></pre>"},{"location":"projects/metaprompt-generator/#updating-appsync","title":"Updating AppSync","text":"<p>Finally, with our generated prompt, we will use the Apollo client to <code>updatePrompt</code> into the GraphQL database.</p> <pre><code>const generatedResponse = await apolloClient.mutate({\n  mutation: UPDATE_PROMPT_MUTATION_WITH_PROMPT,\n  variables: {\n    id: promptId,\n    status: 'GENERATED',\n    prompt: generatedPrompt,\n  },\n});\n</code></pre>"},{"location":"projects/metaprompt-generator/#react-client","title":"React Client","text":"<p>Finally, within the React client, we will subscribe to the AppSync database. Now, when the <code>promptGenerator</code> Lambda updates the AppSync database, the client will also update and the prompt can be shown in the table.</p> <pre><code>useEffect(() =&gt; {\n  const updateSub = client\n    .graphql({ query: subscriptions.promptUpdated })\n    .subscribe({\n      next: ({ data }) =&gt; {\n        console.log(data);\n        const updatedPrompt = data.promptUpdated;\n        if (updatedPrompt) {\n          console.log('Prompt updated:', updatedPrompt);\n          onPromptUpdated({\n            ...updatedPrompt,\n            status:\n              PromptStatus[updatedPrompt.status as keyof typeof PromptStatus],\n          });\n        }\n      },\n      error: (error) =&gt; console.warn(error),\n    });\n\n  return () =&gt; {\n    updateSub.unsubscribe();\n  };\n}, []);\n</code></pre> <p></p>"},{"location":"projects/metaprompt-generator/#prompt-generation","title":"Prompt Generation","text":"<p>Now that we have the infrastructure to asynchronously generate a prompt and return the response to the client, let's see what that prompt looks like.</p> <p>The included prompt can be viewed here. It is a long multi-shot prompt filled with half a dozen examples of good prompts for solving various tasks. These examples help Claude to write a good prompt for your task.</p> <p>Let's see what it can do with an example:</p> <ul> <li>Task: <code>Choose an item from a menu for me given my preferences</code></li> <li>Variables: <code>menu preferences</code></li> <li>Generated Prompt:</li> </ul> <pre><code>You will be helping me choose an item from a restaurant menu based on my preferences and dietary restrictions. I will provide you with two pieces of information:\n\n&lt;menu&gt;\n{MENU}\n&lt;/menu&gt;\n\nThis is the menu, with each menu item on a separate line. Each line contains the dish name followed by a dash and then a description of the dish.\n\n&lt;preferences&gt;\n{PREFERENCES}\n&lt;/preferences&gt;\n\nThis is a description of my preferences and dietary restrictions.\n\nYour task is to carefully review the menu and my preferences, and recommend the menu item that best fits what I'm looking for. Think it through step-by-step in a &lt;scratchpad&gt; section, considering each menu item against my preferences. Explain why you are eliminating options that don't work.\n\nAfter you've thought it through, provide your final recommendation in a &lt;recommendation&gt; section. Please also provide a &lt;justification&gt; explaining why you believe this is the best choice given my preferences. Provide the justification before the final recommendation.\n</code></pre> <p>You can see in this example, that the prompt we will use includes many of the recommended components outlined in Anthropic Prompt Engineering Techniques.</p> <ul> <li>Give Claude a role: Prime Claude to inhabit a specific role (like that of an expert) in order to increase performance for your use case</li> <li>Use XML tags: Incorporate XML tags to structure prompts and responses for greater clarity</li> <li>Let Claude think: Encourage step-by-step thinking to improve the quality of Claude\u2019s output</li> </ul>"},{"location":"projects/metaprompt-generator/#deploying","title":"Deploying","text":"<p>To deploy this demo, you must include an Anthropic API Key in a <code>.env</code> file. This API Key will be passed to the <code>promptGenerator</code> Lambda function and used when making requests to the Anthropic SDK.</p> <pre><code>ANTHROPIC_KEY='sk-ant-api03-XXXXXXX`\n</code></pre>"},{"location":"projects/metaprompt-generator/#requirements","title":"Requirements","text":"<ul> <li>yarn installed</li> <li>AWS account configured</li> </ul> <p>Once configured and requirements installed, you can deploy the CDK stack:</p> <pre><code>yarn launch\n</code></pre> <p>The output will include</p> <pre><code>AnthropicMetaPromptGenerator.InfrastructuremetaPromptGeneratorAPIEndpointF4DD58F9 = https://xxxxxxxxx.execute-api.us-east-1.amazonaws.com/prod/\nAnthropicMetaPromptGenerator.promptGeneratorSite = xxxxxxxx.cloudfront.net\nAnthropicMetaPromptGenerator.siteBucket = anthropicmetapromptgenera-sitewebsitebucketbcxxxxxxxxx\n</code></pre> <p>This Cloudfront Distribution can be used to access the React app and try out the prompt generator.</p>"},{"location":"projects/metaprompt-generator/#cleanup","title":"Cleanup","text":"<p>To delete this stack:</p> <pre><code>yarn cdk destroy\n</code></pre>"},{"location":"projects/claude-multimodal-llm-playground/","title":"Claude Multimodal LLM Playground","text":""},{"location":"projects/claude-multimodal-llm-playground/#claude-35-streamlit-multimodal-llm-playground","title":"Claude 3.5 Streamlit Multimodal LLM Playground","text":""},{"location":"projects/claude-multimodal-llm-playground/#overview","title":"Overview","text":"<p>This repository contains a Streamlit-based application for interacting with Claude 3 and 3.5, a powerful language model developed by Anthropic. The application provides a user-friendly interface for exploring the capabilities of Claude 3, supporting both text-only and image+text inputs, custom system prompts, and tool usage (function calling).</p>"},{"location":"projects/claude-multimodal-llm-playground/#application-versions","title":"Application Versions","text":"<p>We offer two versions of this application, each tailored to different deployment scenarios:</p> <ol> <li>Anthropic API Version</li> <li>Amazon Bedrock Version</li> </ol>"},{"location":"projects/claude-multimodal-llm-playground/#anthropic-api-version","title":"Anthropic API Version","text":"<p>This version interacts directly with the Anthropic API. It's suitable for users who have an Anthropic API key and want to use Claude 3 models without additional cloud infrastructure.</p> <p>Learn more about the Anthropic API version</p>"},{"location":"projects/claude-multimodal-llm-playground/#amazon-bedrock-version","title":"Amazon Bedrock Version","text":"<p>This version is designed to work with Claude 3 and 3.5 models through Amazon Bedrock. It's ideal for users who prefer to leverage AWS infrastructure or have existing workflows in the AWS ecosystem.</p> <p>Learn more about the Amazon Bedrock version</p>"},{"location":"projects/claude-multimodal-llm-playground/#key-features","title":"Key Features","text":"<p>Both versions of the application offer:</p> <ul> <li>Support for multiple Claude 3 models (Opus, Sonnet, Haiku)</li> <li>Text-only and image+text query capabilities</li> <li>Customizable system prompts</li> <li>Tool definition for function calling</li> <li>Temperature and max tokens adjustment</li> <li>Multiple text input fields for complex prompts</li> <li>Image upload functionality</li> <li>Display of AI responses in markdown format</li> <li>Visualization of tool use results in JSON format</li> <li>Token usage display</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/#choosing-a-version","title":"Choosing a Version","text":"<ul> <li>Choose the Anthropic API version if you have direct access to the Anthropic API and prefer a standalone solution.</li> <li>Opt for the Amazon Bedrock version if you're using AWS services or need the additional features and integrations provided by Amazon Bedrock.</li> </ul> <p>Please refer to the README files in each respective folder for detailed setup instructions, usage guidelines, and version-specific information.</p>"},{"location":"projects/claude-multimodal-llm-playground/#combine-text-with-vision-to-create-multimodal-inputs-for-claude-35","title":"Combine Text with Vision to create multimodal inputs for Claude 3.5","text":""},{"location":"projects/claude-multimodal-llm-playground/#test-with-your-own-tool-definitions","title":"Test with your own Tool definitions","text":""},{"location":"projects/claude-multimodal-llm-playground/#contributing","title":"Contributing","text":"<p>We welcome contributions to both versions of the application. Please read our CONTRIBUTING.md file for guidelines on how to submit contributions.</p>"},{"location":"projects/claude-multimodal-llm-playground/#license","title":"License","text":"<p>This project is licensed under the MIT-0 License - see the LICENSE.md file for details.</p>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/","title":"Anthropic Multimodal LLM Playground","text":""},{"location":"projects/claude-multimodal-llm-playground/anthropic/#claude-35-streamlit-multimodal-llm-playground","title":"Claude 3.5 Streamlit Multimodal LLM Playground","text":""},{"location":"projects/claude-multimodal-llm-playground/anthropic/#overview","title":"Overview","text":"<p>This Streamlit application provides a comprehensive interface for interacting with Anthropic's Claude 3 AI models. It supports both text-only and image+text inputs, allows for custom system prompts, and enables the use of tools (function calling) with the AI model. The application is designed to showcase the capabilities of Claude 3 models while providing users with a flexible and intuitive way to experiment with different inputs and settings.</p>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#features","title":"Features","text":"<ul> <li>Support for multiple Claude 3 models (Opus, Sonnet, Haiku)</li> <li>Text-only and image+text query capabilities</li> <li>Customizable system prompts</li> <li>Tool definition for function calling</li> <li>Temperature adjustment for output randomness</li> <li>Max tokens setting for controlling response length</li> <li>Multiple text input fields for complex prompts</li> <li>Image upload functionality with size and quantity limitations</li> <li>Display of AI responses in markdown format</li> <li>Visualization of tool use results in JSON format</li> <li>Token usage display for each response</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#quick-start","title":"Quick Start","text":""},{"location":"projects/claude-multimodal-llm-playground/anthropic/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>pip (Python package installer)</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository:    <code>git clone https://github.com/aws-samples/anthropic-on-aws.git    cd claude-multimodal-llm-playground/anthropic</code></p> </li> <li> <p>Install the required packages:    <code>pip install -r requirements.txt</code></p> </li> </ol> <p>If you don't have a <code>requirements.txt</code> file, you can install the necessary packages directly:    <code>pip install streamlit anthropic python-dotenv Pillow</code></p> <ol> <li>Set up your Anthropic API key:</li> <li>Create a <code>.env</code> file in the project root directory</li> <li>Add your Anthropic API key to the file:      <code>ANTHROPIC_API_KEY='your_api_key_here'</code></li> </ol>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#running-the-application","title":"Running the Application","text":"<p>To start the Streamlit app, run the following command in your terminal:</p> <pre><code>streamlit run playground.py\n</code></pre> <p>This will launch the application and open it in your default web browser. If it doesn't open automatically, you can access it by navigating to the URL displayed in the terminal (usually <code>http://localhost:8501</code>).</p>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#inputs","title":"Inputs","text":""},{"location":"projects/claude-multimodal-llm-playground/anthropic/#model-selection","title":"Model Selection","text":"<ul> <li>Choose from available Claude 3 models: Opus, Sonnet, Haiku, and others as they become available.</li> <li>Each model has different capabilities and performance characteristics.</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#temperature","title":"Temperature","text":"<ul> <li>Adjust the randomness of the AI's outputs on a scale from 0.0 to 1.0.</li> <li>Lower values (closer to 0.0) make the output more focused and deterministic.</li> <li>Higher values (closer to 1.0) make the output more diverse and creative.</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#max-tokens","title":"Max Tokens","text":"<ul> <li>Set the maximum number of tokens for the AI's response.</li> <li>Range: 1 to 4096 tokens.</li> <li>Default value: 4000 tokens.</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#system-prompt","title":"System Prompt","text":"<ul> <li>Optional input to set the context or behavior for the AI.</li> <li>Can be used to give the AI specific instructions or role-play scenarios.</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#tool-definition","title":"Tool Definition","text":"<ul> <li>Optional JSON configuration for function calling.</li> <li>Allows the AI to use external tools or functions during its reasoning process.</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#tool-configuration-example","title":"Tool Configuration Example","text":"<pre><code>{\n  \"name\": \"get_weather\",\n  \"description\": \"Get the current weather in a given location\",\n  \"input_schema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"location\": {\n        \"type\": \"string\",\n        \"description\": \"The city and state, e.g. San Francisco, CA\"\n      }\n    },\n    \"required\": [\"location\"]\n  }\n}\n</code></pre> <p>This example defines a <code>get_weather</code> tool that the AI can use to retrieve weather information for a specified location.</p>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#text-inputs","title":"Text Inputs","text":"<ul> <li>Two text fields are provided for both text-only and image+text queries.</li> <li>Inputs are concatenated if both fields are used, allowing for more complex prompts.</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#image-upload","title":"Image Upload","text":"<ul> <li>Upload images for image+text queries.</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#image-api-limitations","title":"Image API Limitations","text":"<ul> <li>Maximum of 20 files can be uploaded in a single session.</li> <li>Each image file size is limited to 5MB.</li> <li>Supported formats: JPG, JPEG, PNG, GIF, WEBP.</li> <li>The application will display error messages for files exceeding these limitations.</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#application-logic","title":"Application Logic","text":"<ol> <li>Initialization:</li> <li>The application sets up the Anthropic API client using the provided API key.</li> <li> <p>Streamlit interface is initialized with all input fields and settings.</p> </li> <li> <p>User Input:</p> </li> <li>Users select a Claude 3 model, adjust the temperature, and set max tokens.</li> <li>Optional system prompt and tool definition can be provided.</li> <li>For text-only queries, users can input up to two text prompts.</li> <li> <p>For image+text queries, users can upload images and provide up to two text prompts.</p> </li> <li> <p>Input Processing:</p> </li> <li>Text inputs are concatenated if both fields are used.</li> <li>Uploaded images are validated against size and quantity limitations.</li> <li> <p>Tool definitions are parsed from JSON to the required format.</p> </li> <li> <p>API Interaction:</p> </li> <li>For text-only queries, the application sends the processed prompt to the selected Claude model.</li> <li>For image+text queries, each valid image is processed with the concatenated text prompts.</li> <li> <p>System prompts, max tokens, and tool definitions are included in the API call if provided.</p> </li> <li> <p>Response Handling:</p> </li> <li>The application receives the AI's response and processes it for display.</li> <li>Text responses are converted to markdown format for better readability.</li> <li>Tool use results are extracted and displayed in JSON format.</li> <li> <p>Token usage information is displayed for each response.</p> </li> <li> <p>Display:</p> </li> <li>The AI's response is displayed in the main Streamlit interface.</li> <li>The sidebar shows the selected model, temperature, max tokens, and last used prompt.</li> <li>Any error messages (e.g., for invalid tool definitions) are displayed prominently.</li> </ol>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#key-functions","title":"Key Functions","text":""},{"location":"projects/claude-multimodal-llm-playground/anthropic/#get_claude_response","title":"get_claude_response","text":"<p><code>get_claude_response(prompt, model, max_tokens, system_prompt=None, temperature=0.7, tools=None)</code> - Purpose: Handles text-only queries to the Claude model. - Inputs:   - <code>prompt</code>: The user's input text (concatenated if two fields are used).   - <code>model</code>: The selected Claude 3 model ID.   - <code>max_tokens</code>: Maximum number of tokens for the response.   - <code>system_prompt</code>: Optional system instructions for the AI.   - <code>temperature</code>: The randomness setting for the output.   - <code>tools</code>: Optional list of tool definitions for function calling. - Process:   - Constructs the API request with the given parameters.   - Sends the request to the Anthropic API. - Output: Returns the model's response object.</p>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#get_claude_response_image","title":"get_claude_response_image","text":"<p><code>get_claude_response_image(prompt, image, filename, model, max_tokens, system_prompt=None, temperature=0.7, tools=None)</code> - Purpose: Processes image+text queries to the Claude model. - Inputs:   - Similar to <code>get_claude_response</code>, with additional <code>image</code> and <code>filename</code> parameters. - Process:   - Converts the image to base64 format for API submission.   - Constructs the API request with the given parameters and image data.   - Sends the request to the Anthropic API. - Output: Returns the model's response object.</p>"},{"location":"projects/claude-multimodal-llm-playground/anthropic/#to_markdown","title":"to_markdown","text":"<p><code>to_markdown(text)</code> - Purpose: Converts the AI's text response to a markdown format for better display in Streamlit. - Input: Raw text from the AI's response. - Process: Applies markdown formatting, particularly for bullet points. - Output: Returns the formatted text as a string.</p> <p>The application uses these functions within the Streamlit interface to provide a user-friendly way to interact with Claude 3 models. It supports various input types, handles complex queries, and displays the AI's responses in a readable and visually appealing format.</p>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/","title":"Bedrock Multimodal LLM Playground","text":""},{"location":"projects/claude-multimodal-llm-playground/bedrock/#claude-35-streamlit-multimodal-llm-playground","title":"Claude 3.5 Streamlit Multimodal LLM Playground","text":""},{"location":"projects/claude-multimodal-llm-playground/bedrock/#overview","title":"Overview","text":"<p>This Streamlit application provides a comprehensive interface for interacting with Amazon Bedrock's Claude 3 AI models. It supports both text-only and image+text inputs, allows for custom system prompts, and enables the use of tools (function calling) with the AI model. The application is designed to showcase the capabilities of Claude 3 models on Amazon Bedrock while providing users with a flexible and intuitive way to experiment with different inputs and settings.</p>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/#features","title":"Features","text":"<ul> <li>Support for multiple Claude 3 models (Opus, Sonnet, Haiku) via Amazon Bedrock</li> <li>Text-only and image+text query capabilities</li> <li>Customizable system prompts</li> <li>Tool definition for function calling</li> <li>Temperature adjustment for output randomness</li> <li>Multiple text input fields for complex prompts</li> <li>Image upload functionality with size and quantity limitations</li> <li>Display of AI responses in markdown format</li> <li>Visualization of tool use results in JSON format</li> <li>Token usage display for each response</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/#quick-start","title":"Quick Start","text":""},{"location":"projects/claude-multimodal-llm-playground/bedrock/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>pip (Python package installer)</li> <li>AWS account with access to Amazon Bedrock</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository:    <code>git clone https://github.com/aws-samples/anthropic-on-aws.git    cd claude-multimodal-llm-playground/bedrock</code></p> </li> <li> <p>Install the required packages:    <code>pip install -r requirements.txt</code></p> </li> </ol> <p>If you don't have a <code>requirements.txt</code> file, you can install the necessary packages directly:    <code>pip install streamlit boto3 botocore Pillow</code></p> <ol> <li>Set up your AWS credentials:</li> <li>Configure your AWS CLI with <code>aws configure</code> or</li> <li>Set up environment variables:      <code>AWS_ACCESS_KEY_ID=your_access_key_here      AWS_SECRET_ACCESS_KEY=your_secret_key_here      AWS_DEFAULT_REGION=your_preferred_region</code></li> </ol>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/#running-the-application","title":"Running the Application","text":"<p>To start the Streamlit app, run the following command in your terminal:</p> <pre><code>streamlit run bedrock_app.py\n</code></pre> <p>This will launch the application and open it in your default web browser. If it doesn't open automatically, you can access it by navigating to the URL displayed in the terminal (usually <code>http://localhost:8501</code>).</p>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/#inputs","title":"Inputs","text":""},{"location":"projects/claude-multimodal-llm-playground/bedrock/#model-selection","title":"Model Selection","text":"<ul> <li>Choose from available Claude 3 models on Amazon Bedrock: Opus, Sonnet, Haiku, and others as they become available.</li> <li>Each model has different capabilities and performance characteristics.</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/#temperature","title":"Temperature","text":"<ul> <li>Adjust the randomness of the AI's outputs on a scale from 0.0 to 1.0.</li> <li>Lower values (closer to 0.0) make the output more focused and deterministic.</li> <li>Higher values (closer to 1.0) make the output more diverse and creative.</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/#system-prompt","title":"System Prompt","text":"<ul> <li>Optional input to set the context or behavior for the AI.</li> <li>Can be used to give the AI specific instructions or role-play scenarios.</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/#tool-definition","title":"Tool Definition","text":"<ul> <li>Optional JSON configuration for function calling.</li> <li>Allows the AI to use external tools or functions during its reasoning process.</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/#tool-configuration-example","title":"Tool Configuration Example","text":"<pre><code>{\n  \"toolSpec\": {\n    \"name\": \"get_weather\",\n    \"description\": \"Get the current weather in a given location\",\n    \"inputSchema\": {\n      \"json\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"string\",\n            \"description\": \"The city and state, e.g. San Francisco, CA\"\n          }\n        },\n        \"required\": [\"location\"]\n      }\n    }\n  }\n}\n</code></pre> <p>This example defines a <code>get_weather</code> tool that the AI can use to retrieve weather information for a specified location.</p>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/#text-inputs","title":"Text Inputs","text":"<ul> <li>Two text fields are provided for both text-only and image+text queries.</li> <li>Inputs are concatenated if both fields are used, allowing for more complex prompts.</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/#image-upload","title":"Image Upload","text":"<ul> <li>Upload images for image+text queries.</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/#image-api-limitations","title":"Image API Limitations","text":"<ul> <li>Maximum of 20 files can be uploaded in a single session.</li> <li>Each image file size is limited to 5MB.</li> <li>Supported formats: JPG, JPEG, PNG, GIF, WEBP.</li> <li>The application will display error messages for files exceeding these limitations.</li> </ul>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/#application-logic","title":"Application Logic","text":"<ol> <li>Initialization:</li> <li>The application sets up the Amazon Bedrock client using AWS credentials.</li> <li> <p>Streamlit interface is initialized with all input fields and settings.</p> </li> <li> <p>User Input:</p> </li> <li>Users select a Claude 3 model and adjust the temperature.</li> <li>Optional system prompt and tool definition can be provided.</li> <li>For text-only queries, users can input up to two text prompts.</li> <li> <p>For image+text queries, users can upload images and provide up to two text prompts.</p> </li> <li> <p>Input Processing:</p> </li> <li>Text inputs are concatenated if both fields are used.</li> <li>Uploaded images are validated against size and quantity limitations.</li> <li> <p>Tool definitions are parsed from JSON to the required format.</p> </li> <li> <p>API Interaction:</p> </li> <li>For text-only queries, the application sends the processed prompt to the selected Claude model via Amazon Bedrock.</li> <li>For image+text queries, each valid image is processed with the concatenated text prompts.</li> <li> <p>System prompts and tool definitions are included in the API call if provided.</p> </li> <li> <p>Response Handling:</p> </li> <li>The application receives the AI's response and processes it for display.</li> <li>Text responses are converted to markdown format for better readability.</li> <li> <p>Tool use results are extracted and displayed in JSON format.</p> </li> <li> <p>Display:</p> </li> <li>The AI's response is displayed in the main Streamlit interface.</li> <li>The sidebar shows the selected model, temperature, and last used prompt.</li> <li>Any error messages (e.g., for invalid tool definitions) are displayed prominently.</li> <li>Token usage and stop reason are displayed for each response.</li> </ol>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/#key-functions","title":"Key Functions","text":""},{"location":"projects/claude-multimodal-llm-playground/bedrock/#get_bedrock_response","title":"get_bedrock_response","text":"<p><code>get_bedrock_response(prompt, model, system_prompt=None, temperature=0.7, tools=None)</code> - Purpose: Handles text-only queries to the Claude model via Amazon Bedrock. - Inputs:   - <code>prompt</code>: The user's input text (concatenated if two fields are used).   - <code>model</code>: The selected Claude 3 model ID.   - <code>system_prompt</code>: Optional system instructions for the AI.   - <code>temperature</code>: The randomness setting for the output.   - <code>tools</code>: Optional list of tool definitions for function calling. - Process:   - Constructs the API request with the given parameters.   - Sends the request to the Amazon Bedrock API. - Output: Returns the model's response object.</p>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/#get_bedrock_response_image","title":"get_bedrock_response_image","text":"<p><code>get_bedrock_response_image(prompt, image, filename, model, system_prompt=None, temperature=0.7, tools=None)</code> - Purpose: Processes image+text queries to the Claude model via Amazon Bedrock. - Inputs:   - Similar to <code>get_bedrock_response</code>, with additional <code>image</code> and <code>filename</code> parameters. - Process:   - Converts the image to the appropriate format for API submission.   - Constructs the API request with the given parameters and image data.   - Sends the request to the Amazon Bedrock API. - Output: Returns the model's response object.</p>"},{"location":"projects/claude-multimodal-llm-playground/bedrock/#to_markdown","title":"to_markdown","text":"<p><code>to_markdown(text)</code> - Purpose: Converts the AI's text response to a markdown format for better display in Streamlit. - Input: Raw text from the AI's response. - Process: Applies markdown formatting, particularly for bullet points. - Output: Returns the formatted text as a string.</p> <p>The application uses these functions within the Streamlit interface to provide a user-friendly way to interact with Claude 3 models on Amazon Bedrock. It supports various input types, handles complex queries, and displays the AI's responses in a readable and visually appealing format.</p>"},{"location":"projects/complex-schema-tool-use/","title":"Complex Schema Tool Use","text":""},{"location":"projects/complex-schema-tool-use/#using-complex-json-schema-with-claude-3-tools","title":"Using complex JSON Schema with Claude 3 Tools","text":"<p>In these demos, we will show how to use complex JSON schema with Claude 3 using the tools use feature. This is meant as a simple and customizable demonstration you can use to familiarize yourself with how tools work. A basic understanding of tools use on Claude will be important for understanding this demo.</p> <p> Image from Streamlit example</p> <p>From here the bot will use the tool use feature of Claude to walk through the process of ordering a pizza. In this demo, we're not actually going to make a function call so the output will be the JSON that Claude thinks it needs to provide to call the function.</p>"},{"location":"projects/complex-schema-tool-use/#json-schema","title":"JSON Schema","text":"<p>The tool use feature uses JSON schema to define the expected parameters of the tool. This means that Claude knows the schema that is is expected to provide the tool and will do it's best to provide that.</p> <p>This can include simple strings:</p> <pre><code>\"delivery_instructions\": {\n    \"type\": \"string\",\n    \"description\": \"Any special delivery instructions for the order\"\n},\n</code></pre> <p>Enumerated values:</p> <pre><code>\"size\": {\n    \"type\": \"string\",\n    \"description\": \"The size of the pizza\",\n    \"enum\": [\n        \"small\",\n        \"medium\",\n        \"large\",\n        \"extra-large\"\n    ]\n},\n</code></pre> <p>Arrays:</p> <pre><code>\"toppings\": {\n    \"type\": \"array\",\n    \"description\": \"A list of toppings for the pizza\",\n    \"items\": {\n        \"type\": \"string\",\n        \"enum\": [\n            \"pepperoni\",\n            \"sausage\",\n            \"mushrooms\",\n            \"onions\",\n            \"peppers\",\n            \"olives\",\n            \"extra cheese\",\n            \"ham\",\n            \"pineapple\",\n            \"bacon\",\n            \"anchovies\",\n            \"chicken\",\n            \"tomatoes\",\n            \"garlic\",\n            \"spinach\"\n        ]\n    }\n},\n</code></pre> <p>And nested objects:</p> <pre><code>\"customer_details\": {\n    \"type\": \"object\",\n    \"description\": \"Details about the customer placing the order\",\n    \"properties\": {\n        \"name\": {\n            \"type\": \"string\",\n            \"description\": \"The name of the customer\"\n        },\n        \"phone\": {\n            \"type\": \"string\",\n            \"description\": \"The phone number of the customer\"\n        },\n        \"address\": {\n            \"type\": \"string\",\n            \"description\": \"The delivery address for the order\"\n        }\n    },\n    \"required\": [\n        \"name\",\n        \"phone\",\n        \"address\"\n    ]\n}\n</code></pre> <p>The complete schema can be found here</p>"},{"location":"projects/complex-schema-tool-use/#tool-use-best-practices","title":"Tool Use Best Practices","text":"<p>When combined with the above schema, Claude will know what fields are required and examples of what to look for in a response. Unlike other prompts for Claude which rely on examples to guide Claude, when using tools, the description is one of the most important pieces of information. These descriptions should be applied for each component of the JSON schema.</p>"},{"location":"projects/complex-schema-tool-use/#prompting","title":"Prompting","text":"<p>Finally, armed with these tools, we will pass a system prompt and messages to Amazon Bedrock using the Converse API.</p> <pre><code>system_prompt_with_date = (\n    system_prompt\n    + \"\\nThe current date and time is \"\n    + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n)\nconverse_api_params = {\n    \"modelId\": MODEL_ID,\n    \"messages\": st.session_state.messages,\n    \"system\": [{\"text\": system_prompt_with_date}],\n    \"inferenceConfig\": {\n        \"temperature\": TEMPERATURE,\n        \"maxTokens\": MAX_TOKENS,\n    },\n    \"toolConfig\": tools,\n}\nresponse = bedrock_client.converse(**converse_api_params)\n</code></pre> <p>Streamlit code example</p>"},{"location":"projects/complex-schema-tool-use/#result","title":"Result","text":"<p>The result is a simple chat bot that can capture a wide variety of information from a user and produce reliable JSON for further processing.</p> <p> Image from Streamlit example</p> <pre><code>{\n  \"output\": {\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"text\": \"&lt;thinking&gt;\\n- The user has provided all the required information to complete their pizza order:\\n  - Crust: Gluten-free\\n  - Size: Medium\\n  - Toppings: Pepperoni, Sausage\\n  - Extras: None\\n  - Customer Name: Court Schuett\\n  - Customer Phone: 815-555-1212\\n  - Delivery Address: 123 Main St.\\n- I now have everything needed from the data model, so I can invoke the tool to place the order.\\n&lt;/thinking&gt;\"\n        },\n        {\n          \"toolUse\": {\n            \"toolUseId\": \"tooluse_tLSqQR3LQriLD_4P6QgyhA\",\n            \"name\": \"create_pizza_order\",\n            \"input\": {\n              \"crust\": \"gluten-free\",\n              \"size\": \"medium\",\n              \"toppings\": [\"pepperoni\", \"sausage\"],\n              \"extras\": [],\n              \"customer_details\": {\n                \"name\": \"Court Schuett\",\n                \"phone\": \"815-555-1212\",\n                \"address\": \"123 Main St.\"\n              }\n            }\n          }\n        }\n      ]\n    }\n  },\n  \"stopReason\": \"tool_use\"\n}\n</code></pre>"},{"location":"projects/complex-schema-tool-use/nextjs-typescript-example/","title":"NextJS Typescript Example","text":""},{"location":"projects/complex-schema-tool-use/nextjs-typescript-example/#using-this-demo","title":"Using this demo","text":"<p>To get started, you will need the following:</p> <ul> <li>AWS CLI Installed and Configured</li> <li>Amazon Bedrock Model Access for Claude 3 Sonnet</li> <li>Yarn Installed</li> </ul> <p>With these installed and configured, you can clone this repo.</p> <pre><code>git clone https://github.com/aws-samples/anthropic-on-aws.git\ncd anthropic-on-aws/complex-schema-tool-use/nextjs-typescript-example\n</code></pre> <p>Install packages and run the app.</p> <pre><code>yarn\nyarn dev\n</code></pre> <p>You should be able to find your app at: <code>http://localhost:3000</code> or something similar. To get started, tell the bot you'd like to order a pizza.</p>"},{"location":"projects/complex-schema-tool-use/streamlit-python-example/","title":"Streamlit Python Example","text":""},{"location":"projects/complex-schema-tool-use/streamlit-python-example/#using-this-demo","title":"Using this demo","text":"<p>To get started, you will need the following:</p> <ul> <li>AWS CLI Installed and Configured</li> <li>Amazon Bedrock Model Access for Claude 3 Sonnet</li> <li>Python Installed</li> </ul> <p>With these installed and configured, you can clone this repo.</p> <pre><code>git clone https://github.com/aws-samples/anthropic-on-aws.git\ncd anthropic-on-aws/complex-schema-tool-use/streamlit-python-example\n</code></pre> <p>And create a virtual environment and install the necessary packages.</p> <pre><code>python3 -m venv ./\nsource bin/activate\npip3 install -r requirements.txt\n</code></pre> <p>And now run the Streamlit App</p> <pre><code>streamlit run tool_use/chatbot_example.py\n</code></pre> <p>This should open a browser to <code>http://localhost:8501/</code> or something similar. To get started, tell the bot you'd like to order a pizza.</p>"},{"location":"projects/complex-schema-tool-use/streamlit-stepfunction-example/","title":"Streamlit Stepfunction Example","text":""},{"location":"projects/complex-schema-tool-use/streamlit-stepfunction-example/#implementation","title":"Implementation","text":"<p>Streamlit app invokes Step Functions workflow directly everytime user enters input in the textbox. Workflow invokes Amazon Bedrock converse API, identifies if a tool is needed to be called, routes to appropriate tools, builds the response and sends back to the streamlit app. We used Step Functions Express workflow. Express workflows are suitable for high scale short lived transaction type of use cases. They can be called synchronously.</p> <p></p>"},{"location":"projects/complex-schema-tool-use/streamlit-stepfunction-example/#using-this-demo","title":"Using this demo","text":"<p>To get started, you will need the following:</p> <ul> <li>AWS CLI Installed and Configured</li> <li>Amazon Bedrock Model Access for Claude 3 Sonnet</li> <li>Python Installed</li> <li>SAM CLI Installed</li> <li>Streamlit</li> </ul> <p>With these installed and configured, you can clone this repo.</p> <pre><code>git clone https://github.com/aws-samples/anthropic-on-aws.git\ncd anthropic-on-aws/complex-schema-tool-use/streamlit-stepfunction-example\n</code></pre> <p>And create a virtual environment and install the necessary packages.</p> <pre><code>python3 -m venv ./\nsource bin/activate\npip3 install -r requirements.txt\n\n</code></pre> <p>And deploy the cloudformation stack that creates the Step Functions</p> <pre><code>sam build\nsam deploy -g\n</code></pre> <p>And now run the Streamlit App Copy the value for ToolUseCoverseAPIStateMachineArn from the output of the cloudformation stack. Substitute it in the command below</p> <pre><code>streamlit run tool_use/chatbot_example.py &lt;Statemachine_ARN&gt;\n</code></pre> <p>This should open a browser to <code>http://localhost:8501/</code> or something similar. To get started, tell the bot you'd like to order a pizza.</p>"}]}